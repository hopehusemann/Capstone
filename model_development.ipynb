{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa6e6ee8-fed1-4e3c-9f21-a06b77ac34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51197376-a632-41d9-a8bb-cdb9052b0891",
   "metadata": {},
   "source": [
    "# Model Development:\n",
    "Based on the delay distribution, choose appropriate algorithms \n",
    "Train multiple machine learning models, such as:\n",
    "Regression Models: Linear Regression, Random Forest Regression.\n",
    "Classification Models: Logistic Regression, Random Forest Classifier, Gradient Boosting. Tune Hyperparameters: Use GridSearchCV or RandomizedSearchCV to tune hyperparameters for the RandomForestClassifier.\n",
    "Use metrics such as RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), Accuracy, Precision, Recall, F1 Score to evaluate model performance.\n",
    "Model Deployment and Evaluation:\n",
    "Deploy the best performing model on a separate hold-out test data set for final real-world performance evaluation.\n",
    "Analyze the model's prediction accuracy and identify potential areas for improvement.\n",
    "Explore incorporating additional data sources to further enhance prediction performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbbc5e2-0a02-4c1a-9423-9993f075c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\preprocessed_data.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e8c096-cf19-49c6-b832-01d9edf33c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'day_of_week', 'date', 'op_unique_carrier', 'tail_num',\n",
       "       'op_carrier_fl_num', 'origin_iata', 'origin_city', 'dest_iata',\n",
       "       'dest_city', 'crs_dep_time', 'dep_time', 'taxi_out', 'wheels_off',\n",
       "       'wheels_on', 'taxi_in', 'crs_arr_time', 'arr_time', 'cancelled',\n",
       "       'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time',\n",
       "       'flights', 'distance', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
       "       'security_delay', 'late_aircraft_delay', 'origin_state', 'dest_state',\n",
       "       'origin_latitude', 'origin_longitude', 'dest_latitude',\n",
       "       'dest_longitude', 'type_of_airport', 'elevation_ft', 'municipality',\n",
       "       'scheduled_service', 'length_ft', 'width_ft', 'surface', 'lighted',\n",
       "       'closed', 'le_displaced_threshold_ft', 'he_displaced_threshold_ft',\n",
       "       'latitude_orig', 'longitude_orig', 'elevation_orig', 'prcp_orig',\n",
       "       'snow_orig', 'snwd_orig', 'tmax_orig', 'tmin_orig', 'weather_indicator',\n",
       "       'airport_indicator', 'total_delay_time', 'is_weekend', 'arrival_delay',\n",
       "       'departure_delay', 'airline_name', 'eras_tour', 'nfl_game',\n",
       "       'delay_columns', 'latitude_dest', 'longitude_dest', 'elevation_dest',\n",
       "       'prcp_dest', 'snow_dest', 'snwd_dest', 'tmax_dest', 'tmin_dest',\n",
       "       'delay_in_min', 'flight_delay_category', 'delay_time_min', 'year',\n",
       "       'month', 'day', 'is_tour_window'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddf8d49-1f2f-44a9-850c-3ff4b417ed29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>date</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>tail_num</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin_iata</th>\n",
       "      <th>origin_city</th>\n",
       "      <th>dest_iata</th>\n",
       "      <th>dest_city</th>\n",
       "      <th>...</th>\n",
       "      <th>snwd_dest</th>\n",
       "      <th>tmax_dest</th>\n",
       "      <th>tmin_dest</th>\n",
       "      <th>delay_in_min</th>\n",
       "      <th>flight_delay_category</th>\n",
       "      <th>delay_time_min</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>is_tour_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>9e</td>\n",
       "      <td>n131ev</td>\n",
       "      <td>4888</td>\n",
       "      <td>cvg</td>\n",
       "      <td>cincinnati</td>\n",
       "      <td>msp</td>\n",
       "      <td>minneapolis</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>early</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>oo</td>\n",
       "      <td>n905ev</td>\n",
       "      <td>5331</td>\n",
       "      <td>sbn</td>\n",
       "      <td>south bend</td>\n",
       "      <td>ord</td>\n",
       "      <td>chicago</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on_time</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>oo</td>\n",
       "      <td>n908ev</td>\n",
       "      <td>4761</td>\n",
       "      <td>psp</td>\n",
       "      <td>palm springs</td>\n",
       "      <td>lax</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>oo</td>\n",
       "      <td>n908ev</td>\n",
       "      <td>4761</td>\n",
       "      <td>psp</td>\n",
       "      <td>palm springs</td>\n",
       "      <td>lax</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>oo</td>\n",
       "      <td>n908ev</td>\n",
       "      <td>5529</td>\n",
       "      <td>lax</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>rdd</td>\n",
       "      <td>redding</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  day_of_week        date op_unique_carrier tail_num  \\\n",
       "0           0            0  2023-05-01                9e   n131ev   \n",
       "1           1            0  2023-05-01                oo   n905ev   \n",
       "2           2            0  2023-05-01                oo   n908ev   \n",
       "3           3            0  2023-05-01                oo   n908ev   \n",
       "4           4            0  2023-05-01                oo   n908ev   \n",
       "\n",
       "   op_carrier_fl_num origin_iata   origin_city dest_iata    dest_city  ...  \\\n",
       "0               4888         cvg    cincinnati       msp  minneapolis  ...   \n",
       "1               5331         sbn    south bend       ord      chicago  ...   \n",
       "2               4761         psp  palm springs       lax  los angeles  ...   \n",
       "3               4761         psp  palm springs       lax  los angeles  ...   \n",
       "4               5529         lax   los angeles       rdd      redding  ...   \n",
       "\n",
       "   snwd_dest  tmax_dest  tmin_dest  delay_in_min  flight_delay_category  \\\n",
       "0        0.0       73.0       54.0           0.0                  early   \n",
       "1        0.0       73.0       54.0           0.0                on_time   \n",
       "2        0.0       73.0       54.0          13.0                   13.0   \n",
       "3        0.0       73.0       54.0          13.0                   13.0   \n",
       "4        0.0       73.0       54.0           8.0                    8.0   \n",
       "\n",
       "   delay_time_min  year  month  day  is_tour_window  \n",
       "0             0.0  2023      5    1               0  \n",
       "1             0.0  2023      5    1               0  \n",
       "2            13.0  2023      5    1               0  \n",
       "3            13.0  2023      5    1               0  \n",
       "4             8.0  2023      5    1               0  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a830623e-378d-48b4-b3c5-e510bf35a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'date' to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3c6ccc-e73e-4756-b3c0-83a62ed3e804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " Unnamed: 0           0\n",
      "day_of_week          0\n",
      "date                 0\n",
      "op_unique_carrier    0\n",
      "tail_num             0\n",
      "                    ..\n",
      "delay_time_min       0\n",
      "year                 0\n",
      "month                0\n",
      "day                  0\n",
      "is_tour_window       0\n",
      "Length: 80, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "310eb5ec-297c-48fe-80e1-2e9357786ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'day_of_week', 'date', 'op_unique_carrier', 'tail_num',\n",
       "       'op_carrier_fl_num', 'origin_iata', 'origin_city', 'dest_iata',\n",
       "       'dest_city', 'crs_dep_time', 'dep_time', 'taxi_out', 'wheels_off',\n",
       "       'wheels_on', 'taxi_in', 'crs_arr_time', 'arr_time', 'cancelled',\n",
       "       'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time',\n",
       "       'flights', 'distance', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
       "       'security_delay', 'late_aircraft_delay', 'origin_state', 'dest_state',\n",
       "       'origin_latitude', 'origin_longitude', 'dest_latitude',\n",
       "       'dest_longitude', 'type_of_airport', 'elevation_ft', 'municipality',\n",
       "       'scheduled_service', 'length_ft', 'width_ft', 'surface', 'lighted',\n",
       "       'closed', 'le_displaced_threshold_ft', 'he_displaced_threshold_ft',\n",
       "       'latitude_orig', 'longitude_orig', 'elevation_orig', 'prcp_orig',\n",
       "       'snow_orig', 'snwd_orig', 'tmax_orig', 'tmin_orig', 'weather_indicator',\n",
       "       'airport_indicator', 'total_delay_time', 'is_weekend', 'arrival_delay',\n",
       "       'departure_delay', 'airline_name', 'eras_tour', 'nfl_game',\n",
       "       'delay_columns', 'latitude_dest', 'longitude_dest', 'elevation_dest',\n",
       "       'prcp_dest', 'snow_dest', 'snwd_dest', 'tmax_dest', 'tmin_dest',\n",
       "       'delay_in_min', 'flight_delay_category', 'delay_time_min', 'year',\n",
       "       'month', 'day', 'is_tour_window'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "307a0472-9237-46f3-9b18-b34f186360e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])  # Convert 'date' to datetime if it's not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c568668-b107-45e8-9764-85cabb417510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  day_of_week       date op_unique_carrier tail_num  \\\n",
      "2           2            0 2023-05-01                oo   n908ev   \n",
      "3           3            0 2023-05-01                oo   n908ev   \n",
      "4           4            0 2023-05-01                oo   n908ev   \n",
      "5           5            0 2023-05-01                oo   n908ev   \n",
      "6           6            0 2023-05-01                oo   n908ev   \n",
      "\n",
      "   op_carrier_fl_num origin_iata   origin_city dest_iata    dest_city  ...  \\\n",
      "2               4761         psp  palm springs       lax  los angeles  ...   \n",
      "3               4761         psp  palm springs       lax  los angeles  ...   \n",
      "4               5529         lax   los angeles       rdd      redding  ...   \n",
      "5               5529         lax   los angeles       rdd      redding  ...   \n",
      "6               5529         lax   los angeles       rdd      redding  ...   \n",
      "\n",
      "   snwd_dest  tmax_dest  tmin_dest  delay_in_min  flight_delay_category  \\\n",
      "2        0.0       73.0       54.0          13.0                   13.0   \n",
      "3        0.0       73.0       54.0          13.0                   13.0   \n",
      "4        0.0       73.0       54.0           8.0                    8.0   \n",
      "5        0.0       73.0       54.0           8.0                    8.0   \n",
      "6        0.0       73.0       54.0           8.0                    8.0   \n",
      "\n",
      "   delay_time_min  year  month  day  is_tour_window  \n",
      "2            13.0  2023      5    1               0  \n",
      "3            13.0  2023      5    1               0  \n",
      "4             8.0  2023      5    1               0  \n",
      "5             8.0  2023      5    1               0  \n",
      "6             8.0  2023      5    1               0  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to show rows where 'delay_in_min' is not zero\n",
    "non_zero_delays = df[df['delay_in_min'] != 0]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "print(non_zero_delays.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd071246-6ca8-4994-81e6-8d6b3307f8c0",
   "metadata": {},
   "source": [
    "Feature Engineering:\n",
    "Created new features like:\n",
    "is_weekend: A binary feature indicating whether the flight is on the weekend (Saturday or Sunday).\n",
    "dep_time: Converted to a datetime object, and the hour (dep_hour) has been extracted.\n",
    "is_morning, is_afternoon, and is_evening: These features are based on the departure time (dep_hour) and help categorize the flights based on the time of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27aad675-ad32-41c7-8e0f-4aed497964a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81f2153b-ed4b-40d3-9215-0d7edd14c169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15118733, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ac2ac85-a002-44f8-a72e-2f975b11cf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'day_of_week', 'date', 'op_unique_carrier', 'tail_num',\n",
       "       'op_carrier_fl_num', 'origin_iata', 'origin_city', 'dest_iata',\n",
       "       'dest_city', 'crs_dep_time', 'dep_time', 'taxi_out', 'wheels_off',\n",
       "       'wheels_on', 'taxi_in', 'crs_arr_time', 'arr_time', 'cancelled',\n",
       "       'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time',\n",
       "       'flights', 'distance', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
       "       'security_delay', 'late_aircraft_delay', 'origin_state', 'dest_state',\n",
       "       'origin_latitude', 'origin_longitude', 'dest_latitude',\n",
       "       'dest_longitude', 'type_of_airport', 'elevation_ft', 'municipality',\n",
       "       'scheduled_service', 'length_ft', 'width_ft', 'surface', 'lighted',\n",
       "       'closed', 'le_displaced_threshold_ft', 'he_displaced_threshold_ft',\n",
       "       'latitude_orig', 'longitude_orig', 'elevation_orig', 'prcp_orig',\n",
       "       'snow_orig', 'snwd_orig', 'tmax_orig', 'tmin_orig', 'weather_indicator',\n",
       "       'airport_indicator', 'total_delay_time', 'is_weekend', 'arrival_delay',\n",
       "       'departure_delay', 'airline_name', 'eras_tour', 'nfl_game',\n",
       "       'delay_columns', 'latitude_dest', 'longitude_dest', 'elevation_dest',\n",
       "       'prcp_dest', 'snow_dest', 'snwd_dest', 'tmax_dest', 'tmin_dest',\n",
       "       'delay_in_min', 'flight_delay_category', 'delay_time_min', 'year',\n",
       "       'month', 'day', 'is_tour_window'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faad0d7e-36d3-4cb9-abc6-be9f641a81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example timestamp column\n",
    "timestamp_columns = df.select_dtypes(include=['datetime']).columns\n",
    "\n",
    "# Convert timestamp columns to numerical values (e.g., extract year, month, day)\n",
    "for col in timestamp_columns:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    df[f'{col}_year'] = df[col].dt.year\n",
    "    df[f'{col}_month'] = df[col].dt.month\n",
    "    df[f'{col}_day'] = df[col].dt.day\n",
    "    df.drop(columns=[col], inplace=True)  # Optionally drop the original timestamp columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34f6db67-c649-457d-a992-076e8f894f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b4fa57f-ecd1-4fac-8205-b069ae0f9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "# We should include only numeric columns for imputation\n",
    "\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "#categorical_columns = ['origin_iata', 'dest_iata', 'month', 'op_unique_carrier', 'day_of_week', 'eras_tour', 'nfl_game']  # Modify with your actual categorical columns\n",
    "#date_columns = ['date']  # Modify with your actual date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2aa1a53d-a791-45e2-95b3-875d47b77840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define the preprocessor (ColumnTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),  # Apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)  # Apply OneHotEncoder to categorical columns\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep any other columns unchanged\n",
    ")\n",
    "\n",
    "# Step 5: Apply preprocessing to the DataFrame (this will transform categorical columns and pass-through numerical ones)\n",
    "X_processed = preprocessor.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee02da-5969-4b77-86ac-bdf3b402fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns that aren't needed for correlation\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea7a28-8aa3-47ea-a8e0-043720313d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numeric columns to a sparse matrix format\n",
    "df_numeric_sparse = csr_matrix(df_numeric)\n",
    "\n",
    "# Now compute the correlation matrix on the sparse matrix\n",
    "correlation_matrix = pd.DataFrame(df_numeric_sparse.toarray()).corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea80316-cb97-42f1-b54e-f7c0cdb40504",
   "metadata": {},
   "source": [
    "Final Data Cleaning Summary\n",
    "After these steps, data is cleaned and ready for modeling. \n",
    "\n",
    "Here's a summary of the main actions:\n",
    "Missing values: Imputed numerical columns (e.g., delay times) with the median, and categorical columns with the mode.\n",
    "Categorical variables: Encoded day_of_week with label encoding and other categorical columns with one-hot encoding.\n",
    "Time variables: Converted time columns into hours and minutes, and added additional time-based features (e.g., is_weekend, is_morning).\n",
    "Dropped irrelevant columns: Removed columns that don’t contribute to the model’s predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a15b2-4623-49c1-95ae-a49179ee9199",
   "metadata": {},
   "source": [
    "For regression tasks (predicting the delay time), we can perform a correlation analysis to see which features are strongly correlated with the target variable (e.g., delay_in_min, total_delay_time, or flight_delay_time). We can also remove highly correlated features to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b1a90-5a2a-401a-bbb4-8d53c97a4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample 10% of the rows using the row indices\n",
    "n_rows = df_numeric_sparse.shape[0]  # Get the number of rows\n",
    "sample_indices = np.random.choice(n_rows, size=int(n_rows * 0.1), replace=False)  # Random row indices\n",
    "\n",
    "# Step 2: Extract the sampled rows from the sparse matrix\n",
    "df_sample_sparse = df_numeric_sparse[sample_indices]\n",
    "\n",
    "# Step 3: Convert the sparse matrix to a dense matrix for correlation calculation\n",
    "df_sample_dense = pd.DataFrame(df_sample_sparse.toarray())\n",
    "\n",
    "# Step 4: Compute the correlation matrix on the sampled data\n",
    "correlation_matrix = df_sample_dense.corr()\n",
    "\n",
    "# Output the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b355a-9e51-46d9-b407-cf48b63deb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ab466-3ade-4e41-8831-10a4bab40b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_matrix.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8244eb3-ae00-4d36-952e-5bdd8c925642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the problematic columns: Check the columns with 54 missing values\n",
    "problematic_columns = correlation_matrix.columns[correlation_matrix.isnull().sum() == 54]\n",
    "print(correlation_matrix[problematic_columns].head())  # Inspect these columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7d143-c60e-4c64-9fb2-6e32ebe64ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with all missing values\n",
    "correlation_matrix_cleaned = correlation_matrix.dropna(axis=1, how='all')\n",
    "print(correlation_matrix_cleaned.shape)  # Check the new shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815a840-7be9-43dd-b384-a81372364717",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eaa2b5-7550-4871-9486-a11b29681ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix to identify strong correlations\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt='.2f', cbar=True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features (threshold > 0.8, for example)\n",
    "corr_threshold = 0.8\n",
    "high_corr_features = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > corr_threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            high_corr_features.add(colname)\n",
    "\n",
    "print(f\"Highly correlated features: {high_corr_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86107abc-2542-43e8-899c-12660412099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of highly correlated feature indices from your output\n",
    "high_corr_indices = {3, 5, 6, 7, 9, 10, 14, 15, 17, 23, 24, 26, 27, 28, 29, 30, \n",
    "                     36, 37, 38, 42, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, \n",
    "                     58, 59, 61, 62, 67}\n",
    "\n",
    "# Extract the column names from the original DataFrame, not the correlation matrix\n",
    "feature_names = df.columns\n",
    "\n",
    "# Map the indices to feature names\n",
    "high_corr_feature_names = [feature_names[i] for i in high_corr_indices]\n",
    "\n",
    "# Print the highly correlated feature names\n",
    "print(f\"Highly correlated features: {high_corr_feature_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8ee24-47e6-4b45-8db5-1fad1f719131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to drop\n",
    "features_to_drop = ['origin_city', 'dest_city', 'taxi_out', 'unique_id', 'municipality']\n",
    "\n",
    "# Remove the specified features from the high correlation feature list\n",
    "high_corr_feature_names_cleaned = [feature for feature in high_corr_feature_names if feature not in features_to_drop]\n",
    "\n",
    "# Add 'eras_tour' to the list of highly correlated features\n",
    "high_corr_feature_names_cleaned.append('eras_tour')\n",
    "\n",
    "# Print the cleaned list of highly correlated features\n",
    "print(f\"Updated list of highly correlated features: {high_corr_feature_names_cleaned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329536fd-e713-4cfe-a728-fd2fc468406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a set to store highly correlated features\n",
    "high_corr_features = set()\n",
    "\n",
    "# Loop through the correlation matrix and find highly correlated features\n",
    "corr_threshold = 0.8\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > corr_threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            high_corr_features.add(colname)\n",
    "\n",
    "# Print the highly correlated features\n",
    "print(f\"Highly correlated features: {high_corr_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82dbe7-8126-4516-8aaf-9c4454559c7a",
   "metadata": {},
   "source": [
    "In the correlation heatmap:\n",
    "\n",
    "Strong correlations (values near 1 or -1) may indicate features that carry redundant information.\n",
    "Drop one of each pair of highly correlated features to reduce redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625ddc6-3d0a-40b4-9c16-70caae7529b1",
   "metadata": {},
   "source": [
    " Feature Importance (for both classification and regression)\n",
    "We can use models like Random Forest to assess feature importance. Random Forest has a built-in mechanism to evaluate how important each feature is for predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97429010-f8d0-4817-86cd-8ca664225f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9036de-8969-4b52-93a7-3302ccf906cf",
   "metadata": {},
   "source": [
    "Wheels On & Arrival Delay: The strongest correlation with arrival_delay is with wheels_on (0.276). This indicates that as the wheels-on time increases, arrival delays may also increase.\n",
    "Departure Delay & Departure Time: There’s a notable positive correlation (0.245) between dep_time and departure_delay, suggesting that later departure times are associated with greater delays.\n",
    "Arrival Delay and Departure Delay: There’s a moderate correlation (0.156) between arrival_delay and departure_delay, which is expected since delays often propagate through a flight schedule.\n",
    "Weather Variables: The correlations with weather variables (prcp, snow, snwd, tmax, tmin) are quite low, indicating that weather may not have a significant impact on delays in this dataset.\n",
    "Other Factors: Variables like crs_dep_time, crs_arr_time, and total_delay_time have negative correlations with delays, which might suggest timing discrepancies play a role.Visualize Relationships: Create scatter plots or pair plots to visualize relationships between key variables, especially those with stronger correlations. This can help identify any non-linear patterns or clusters.\n",
    "\n",
    "\n",
    "Modeling: Building regression models using delay_in_min as target variable. Use features with significant correlations as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b10b30-4ec3-4f6a-a2f2-0ab158d4735d",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "\n",
    "\n",
    "Retain Key Features: Select the features that strongly contribute to the first two principal components (PC1 and PC2). These are the components with the highest feature importance scores.\n",
    "Eliminate Low-Impact Features: If certain features contribute minimally to the important components (e.g., those with very low importance scores), consider removing them to simplify the model and improve computation efficiency.\n",
    "PCA-Driven Feature Subset: After identifying the top features, use this reduced set of features to train the model again and see if it performs as well as the model with all features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791edd4-4e6c-4d25-bbca-e44751bc046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# List of highly correlated feature names\n",
    "high_corr_feature_names = ['op_carrier_fl_num', 'dest_iata', 'dep_time', 'crs_arr_time', 'arr_time', 'diverted', \n",
    "                           'carrier_delay', 'weather_delay', 'security_delay', 'late_aircraft_delay', 'origin_state', \n",
    "                           'dest_state', 'origin_latitude', 'scheduled_service', 'surface', 'closed', \n",
    "                           'le_displaced_threshold_ft', 'latitude_orig', 'longitude_orig', 'elevation_orig', \n",
    "                           'prcp_orig', 'snow_orig', 'snwd_orig', 'tmax_orig', 'iata_dest_dest', 'weather_indicator', \n",
    "                           'delay_in_min', 'is_weekend', 'nfl_game', 'eras_tour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e75f89-f8ee-42e7-ad1d-2dfe81ee1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features (X) and target variable (y)\n",
    "X = df[high_corr_feature_names]\n",
    "y = df['delay_in_min']  \n",
    "\n",
    "# Create a copy of X to avoid slicing issues\n",
    "X = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7d6b7-3537-4a94-b5bc-b7bf96cd9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Convert datetime columns to numeric (e.g., days or seconds since the minimum date)\n",
    "datetime_columns = X.select_dtypes(include=['datetime']).columns\n",
    "\n",
    "for col in datetime_columns:\n",
    "    X[col] = (X[col] - X[col].min())  # Convert datetime to timedelta\n",
    "    X[col] = X[col].dt.total_seconds()  # Convert timedelta to seconds\n",
    "\n",
    "\n",
    "# Step 2: Apply One-Hot Encoding to categorical columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 3: Handle missing values (if any)\n",
    "X_encoded = X_encoded.fillna(0)  # You can also use X_encoded.dropna() if you prefer to remove missing data\n",
    "\n",
    "# Step 4: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Step 5: Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=10)  # Adjust the number of components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the Random Forest model\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions and evaluate the model\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdd2d6-c8bc-4a80-b2fe-e7811910bcf2",
   "metadata": {},
   "source": [
    "A classification accuracy of 94.74% indicates that this model is doing a great job at predicting the target variable (total_delay_time) using the selected features. This suggests that the dimensionality reduction (via PCA) and preprocessing steps, along with the Random Forest classifier, are working well together.\n",
    "\n",
    "Component 1: Features like iata_dest_dest_sux, dest_state_mt, and elevation_orig contribute heavily to the first principal component.\n",
    "Component 2: Weather-related features like tmax_orig, origin_latitude, and prcp_orig are most significant in the second principal component.\n",
    "Component 10: Time-related features such as carrier_delay, late_aircraft_delay, arr_time, and dep_time are dominant in the tenth principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395d9dc-fb2b-4417-bba3-72b1a750353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "feature_importance = model_rf.feature_importances_\n",
    "\n",
    "# Now print the number of feature importances\n",
    "print(f\"Number of feature importances: {len(feature_importance)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbefdc0-985a-4aae-826f-c72217da9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of features in X_encoded: {X_encoded.shape[1]}\")\n",
    "print(f\"Number of feature importances: {len(feature_importance)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518e6e9-6e4f-4005-a453-4521fcc35858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have already fitted your PCA model on the scaled data (X_scaled)\n",
    "# If not, fit PCA first\n",
    "# pca = PCA(n_components=10)\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Get the components (loadings)\n",
    "components = pd.DataFrame(pca.components_, columns=X_encoded.columns)\n",
    "\n",
    "# Show the top features for each component\n",
    "top_features_per_component = {}\n",
    "\n",
    "for i in range(components.shape[0]):\n",
    "    # Get absolute values of the loadings for each component\n",
    "    component_loadings = components.iloc[i].abs()\n",
    "    top_features = component_loadings.nlargest(5).index.tolist()\n",
    "    top_features_per_component[f'Component {i+1}'] = top_features\n",
    "\n",
    "# Display the top features per component\n",
    "for component, features in top_features_per_component.items():\n",
    "    print(f\"{component}: {', '.join(features)}\")\n",
    "\n",
    "# Display the explained variance ratio for each component\n",
    "print(\"\\nExplained Variance Ratio by Component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Now if you want to map feature importance to PCA components based on the explained variance:\n",
    "feature_importance = pca.explained_variance_ratio_  # Use explained variance ratio as feature importance\n",
    "\n",
    "# Create a DataFrame to show the feature importance for each component\n",
    "importance_df = pd.DataFrame({\n",
    "    'PCA Component': [f'PC{i+1}' for i in range(len(feature_importance))],\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "print(\"\\nFeature Importance Mapped to PCA Components:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ad4b9-5518-4123-8e60-348b610d0594",
   "metadata": {},
   "source": [
    "Top Features Contributing to PCA Components:\n",
    "\n",
    "For each principal component (PC), you can see the most important features that contributed to it. For example:\n",
    "Component 1 (PC1): This component is most influenced by features related to the longitude_orig, elevation_orig, and destination-related variables (e.g., dest_iata_sux, iata_dest_dest_sux). These features contribute the most to explaining the variance along this axis.\n",
    "Component 2 (PC2): Features such as tmax_orig, origin_latitude, and weather-related variables (e.g., weather_indicator, prcp_orig) dominate this component.\n",
    "Component 3 (PC3): The late_aircraft_delay, arr_time, and crs_arr_time are heavily loaded on this component, indicating that these features explain variance related to flight timings and delays.\n",
    "Other components reveal similar patterns where certain features such as carrier_delay, weather_indicator, and late_aircraft_delay dominate specific components.\n",
    "\n",
    "\n",
    "Feature Importance Mapped to PCA Components:\n",
    "\n",
    "PC1 explains 42.87% of the variance in the data, making it the most important component in terms of variance captured.\n",
    "PC2 explains 19.42% of the variance, and PC3 explains 9.51%. These three components together account for over 70% of the variance in the dataset.\n",
    "The subsequent components contribute progressively less, with PC10 explaining only 0.58% of the variance.\n",
    "\n",
    "PC1 has the highest importance (0.4287), reflecting its substantial contribution to the overall variance.\n",
    "\n",
    "\n",
    "Summary of Results:\n",
    "\n",
    "Top Contributing Features: These are the features that strongly influence each principal component. For example:\n",
    "\n",
    "PC1: Features like longitude_orig, elevation_orig, and iata_dest_dest_sux are crucial, indicating that geographic and destination-related features play a major role.\n",
    "\n",
    "PC2: Weather features (tmax_orig, prcp_orig) and geographic features like origin_latitude and origin_state_co are important, showing that the geographic and weather conditions contribute to a significant amount of variance in the data.\n",
    "\n",
    "Variance Explanation: PC1 is the most critical component, explaining nearly 43% of the variance in your data, which is a significant portion. The remaining components progressively capture less variance, so PC1 and PC2 likely explain the most significant patterns in your data, while the lower-order components (PC3 through PC10) capture finer, more specific relationships.\n",
    "\n",
    "Feature Importance: The explained variance ratio gives a sense of how important each principal component is in terms of explaining variance in the data. For example, since PC1 explains a large chunk of the variance, features contributing to this component should be considered the most important for understanding the dataset's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b30892-7580-4f17-ada7-b1a8ee23db2d",
   "metadata": {},
   "source": [
    "Principal Components' Importance: The feature importances show how much each principal component contributes to the overall decision-making of the random forest model. Since PCA reduces the feature space, these components are now responsible for capturing the underlying patterns in the data.\n",
    "PC1 and PC10 are the most critical components, so you should pay attention to the features that heavily influence these components.\n",
    "Top Contributing Features: The list of top features for each component tells you what variables are most important for each principal component, helping to understand which parts of your data (e.g., weather data, time data, or location data) are contributing the most to the model's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a566c5d-b506-43ef-be1a-6a619cb7a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction: Since PC1 and PC2 explain a significant portion of the variance, using just the first two or three\n",
    "# principal components for further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fd040-12f8-42c6-b8ae-5bf99a543353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to the first two principal components (PC1, PC2)\n",
    "X_reduced_2d = X_pca[:, :2]\n",
    "\n",
    "# Or, reduce to the first three principal components (PC1, PC2, PC3)\n",
    "X_reduced_3d = X_pca[:, :3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ad3dd-df85-4913-bf67-aceaa2a9bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot of PC1 vs PC2\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced_2d[:, 0], X_reduced_2d[:, 1], alpha=0.5)\n",
    "plt.title(\"2D PCA - PC1 vs PC2\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0f80e-3cf4-4323-bcd5-a5952030e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D scatter plot of PC1, PC2, and PC3\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_reduced_3d[:, 0], X_reduced_3d[:, 1], X_reduced_3d[:, 2], alpha=0.5)\n",
    "ax.set_title(\"3D PCA - PC1, PC2, and PC3\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1c5fe-472f-4e9c-89b5-4b5be48e2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng or visualization (e.g., for clustering, classification, or regression), thereby reducing the dimensionality of your data.\n",
    "\n",
    "Interpretation for Machine Learning: For predictive tasks like classification, understanding which features are most influential in PCA components can help you interpret the model results more effectively. If you're using the reduced features (via PCA), you can see which combinations of features (from the original dataset) are driving the most variance.\n",
    "\n",
    "Visualizing the Components: You can also create a scatter plot of the first two or three principal components to visualize the data's structure in a lower-dimensional space. This can help you gain more insights into the relationships and clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3f622-e460-46f4-a0bf-d50d399f0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform KMeans clustering on the reduced data (PC1 and PC2 or PC1, PC2, PC3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # Choose the number of clusters\n",
    "kmeans.fit(X_reduced_2d)  # Or use X_reduced_3d for 3D clusters\n",
    "\n",
    "# Get the cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Plot the clusters in 2D\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced_2d[:, 0], X_reduced_2d[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "plt.title(\"2D PCA with KMeans Clusters\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296cbdee-6f4f-4b60-9178-012d04487cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting Feature Importance in PCA for Machine Learning\n",
    "# Getting the feature importance (loadings) for each component\n",
    "pca_components = pca.components_\n",
    "\n",
    "# Create a DataFrame of the PCA components (feature loadings)\n",
    "import pandas as pd\n",
    "\n",
    "feature_importance = pd.DataFrame(pca_components.T, columns=[f\"PC{i+1}\" for i in range(pca_components.shape[0])], \n",
    "                                  index=X_encoded.columns)\n",
    "\n",
    "# Display the top features for each principal component\n",
    "print(feature_importance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb8f42-a60b-4aa9-b50d-4bad89ac8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "# Create a heatmap of the feature importances)\r\n",
    "plt.figure(figsize=(12, 8))\r\n",
    "sns.heatmap(feature_importance, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\r\n",
    "plt.title('PCA Fe Importancesdings')\r\n",
    "plt.xlabel('Principal Components')\r\n",
    "plt.ylabel('Features')\r\n",
    "plt.show()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f9de0-d643-4fa2-821b-d0b078080e66",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Principal Component 1 (PC1):\n",
    "Top features: dep_time, crs_arr_time, arr_time\n",
    "Key insights: Time-related features such as departure and arrival times seem to play a significant role in defining PC1.\n",
    "Principal Component 2 (PC2):\n",
    "Top features: op_carrier_fl_num, weather_indicator, prcp_orig, tmax_orig\n",
    "Key insights: This component seems to capture weather-related features like weather_indicator and prcp_orig, along with flight carrier features like op_carrier_fl_num.\n",
    "Principal Component 3 (PC3):\n",
    "Top features: late_aircraft_delay, arr_time, crs_arr_time\n",
    "Key insights: PC3 seems to be primarily associated with time delays, with a focus on arrival times and late aircraft delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff3f81-e690-4cd5-918b-6e561dc5ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Modeling Using Reduced Features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the reduced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy using reduced features: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc4c43-05dc-47d2-b360-51894c9b0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Dimensionality Reduction \n",
    "\n",
    "# Since PC1 and PC2 capture a significant amount of variance (over 60%), focus on these two components for downstream tasks\n",
    "# like clustering, classification, or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f346e-d09e-4955-8408-f0bb954d769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first two principal components for further modeling or visualization\n",
    "X_pca_2d = X_pca[:, :2]  # Two-dimensional representation\n",
    "\n",
    "# Or, keep the first three components\n",
    "X_pca_3d = X_pca[:, :3]  # Three-dimensional representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f22b4-00e8-4e29-8fd0-d1e3961b58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2D Scatter plot of the first two principal components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.5, c=y, cmap='viridis')\n",
    "plt.title('PCA - 2D Scatter Plot (PC1 vs PC2)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Target Variable (y)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b9011-e5ae-49f5-8e3e-5de3e02c1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 3D Scatter plot of the first three principal components\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=y, cmap='viridis')\n",
    "\n",
    "ax.set_title('PCA - 3D Scatter Plot (PC1 vs PC2 vs PC3)')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "fig.colorbar(sc, label='Target Variable (y)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae7135-ac1d-43bf-ad11-5184cda6b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply KMeans clustering on the first two principal components\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_pca_2d)\n",
    "\n",
    "# Add cluster labels to the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title('KMeans Clustering on PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f0f29-c284-46dc-bb7b-187bd0b6e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply KMeans clustering on the first two principal components\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_pca_2d)\n",
    "\n",
    "# Add cluster labels to the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.title('KMeans Clustering on PCA Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010cb5e-0cec-4a9f-979f-d41f017874fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on PCA-reduced data: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931ee98-37e7-40f5-b4ba-8e27d9053c74",
   "metadata": {},
   "source": [
    "## Model Evaluation on Hold-out Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd02ad8-3b87-4dbc-a5d0-1b0ad9dbdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Make predictions on the hold-out test set\n",
    "y_pred = model_rf.predict(X_test)  # Assuming rf_model is your trained RandomForestClassifier\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Precision, Recall, F1 Score\n",
    "precision = precision_score(y_test, y_pred, average='binary')  # 'binary' for binary classification\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# ROC-AUC (only for binary classification)\n",
    "roc_auc = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])  # Probability of positive class\n",
    "print(f'ROC-AUC: {roc_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851cc73-6961-4017-aa84-eb309388ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top contributing features for each component\n",
    "top_features = feature_importance.abs().nlargest(5, 'PC1')  # Top 5 features for PC1\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bc0d7-eaba-47fb-9cc4-ddc69d671293",
   "metadata": {},
   "source": [
    "\n",
    "It seems that the feature loadings (principal component coefficients) are highly uniform across several features, especially the first five columns: longitude_orig, elevation_orig, dest_iata_sux, dest_state_mt, and iata_dest_dest_sux. All of them seem to have the exact same values across all components.\n",
    "\n",
    "Insights and Possible Issues\n",
    "High Similarity Across Features:\n",
    "\n",
    "This pattern suggests that these features are highly correlated with each other. PCA identifies the directions of maximum variance, so if these features have similar values or high correlation, PCA will treat them almost identically.\n",
    "For example, longitude_orig and elevation_orig are often correlated with geographical features, so it makes sense that PCA might group them similarly.\n",
    "\n",
    "\n",
    "Lack of Diversity:\n",
    "\n",
    "Since the feature set is dominated by a small number of correlated features, they will disproportionately influence the principal components. This can make the PCA results less interpretable or overly biased towards certain variables.\n",
    "Features like longitude_orig, elevation_orig, and dest_iata_sux may be more or less identical in terms of their contribution to each principal component, resulting in the loadings being the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcf440-53b9-4c1f-a64b-e801614b3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming df is your DataFrame and y is your target variable\n",
    "\n",
    "# Selecting features contributing most to PC1 and PC2\n",
    "important_features = ['iata_dest_dest', 'dest_state', 'elevation_orig', 'dest_iata', 'latitude_orig', \n",
    "                      'tmax_orig', 'origin_latitude', 'origin_state', 'prcp_orig', 'weather_indicator']\n",
    "\n",
    "# Create a new DataFrame with only the important features\n",
    "X_important = df[important_features]\n",
    "\n",
    "# One-Hot Encoding of categorical columns\n",
    "X_encoded = pd.get_dummies(X_important)\n",
    "\n",
    "# Sample a subset of the data, say 20% of the original dataset\n",
    "sample_fraction = 0.2  # Adjust the fraction as needed (0.2 means 20% sample)\n",
    "df_sampled = X_encoded.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Align target variable 'y' with the sampled features\n",
    "y_sampled = y[df_sampled.index]\n",
    "\n",
    "# Split the sampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sampled, y_sampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model with the selected features\n",
    "model_rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy for the sampled data\n",
    "print(f\"Accuracy with selected features on sampled data: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97524f-6239-4972-8393-cc382060b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0ef44-d76f-4910-8b9f-166a56f44eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the hold-out test set\n",
    "# y_pred = model_rf.predict(X_test)\n",
    "\n",
    "# # Accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# # Precision, Recall, F1 Score (use zero_division parameter to avoid warnings)\n",
    "# precision = precision_score(y_test, y_pred, average='binary', zero_division=1)  # 'binary' for binary classification\n",
    "# recall = recall_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "# f1 = f1_score(y_test, y_pred, average='binary', zero_division=1)\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# print('Confusion Matrix:')\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # ROC-AUC (only for binary classification)\n",
    "# if len(set(y_test)) > 1:  # Check if both classes are present\n",
    "#     roc_auc = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])  # Probability of positive class\n",
    "#     print(f'ROC-AUC: {roc_auc:.4f}')\n",
    "# else:\n",
    "#     print(\"ROC-AUC cannot be calculated because only one class is present in the test set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7747d-abfa-4ceb-8a6c-93fd712a2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase max columns displayed to ensure you see all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display data types of all columns\n",
    "# Display data types of first few columns\n",
    "print(X_train.iloc[:, :50].dtypes)  # Adjust 10 to the number of columns you want to inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2935bb-f96f-44c8-8391-28caf897b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types of first few columns\n",
    "print(X_train.iloc[:, 50:100].dtypes)  # Adjust 10 to the number of columns you want to inspect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed734e07-50e3-4c4c-b1ea-4798bd13bce2",
   "metadata": {},
   "source": [
    "Interpret the plot: Features with higher importance should be kept, and those with low importance can be dropped to improve model performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b5202-d62d-4f6a-adec-0199a2c5afbe",
   "metadata": {},
   "source": [
    "Removing Redundant and Low-Variance Features\n",
    "If some features have little variance (i.e., they don’t change much across the dataset), they may not contribute significantly to the model’s performance. You can remove those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3b8bd-f71a-4c98-aded-7a28098ddb7a",
   "metadata": {},
   "source": [
    "F-statistic: A large F-statistic indicates that the variability explained by the model is significantly greater than the variability unexplained (the residual variance). This suggests that the model fits the data well. p-value: A p-value of 0.0 (essentially) means that the null hypothesis (which states that all coefficients are equal to zero) can be rejected. This suggests that at least one of the predictors is significantly related to the delays. Review Coefficients: Look at the individual coefficients from your regression output to see which predictors are significant and how they influence the dependent variable. Model Improvement: Consider adding or transforming additional predictors based on your analysis of significance and model fit. Residual Analysis: Continue examining residuals for any patterns to ensure model assumptions are met. Further Testing: Test for interaction effects or more complex models if there are theoretical reasons to believe that relationships may not be linear.\n",
    "\n",
    "R-squared and Adjusted R-squared:\n",
    "\n",
    "R-squared: 0.864 suggests that approximately 86.4% of the variability in the adjusted_elapsed_time can be explained by the model. This is a strong indicator of a good fit.\n",
    "Adjusted R-squared: 0.863 adjusts for the number of predictors in the model, indicating that adding or removing predictors doesn't lead to overfitting.\n",
    "\n",
    "Coefficients:\n",
    "\n",
    "Intercept (const): 48.0038 suggests the baseline time when all predictors are zero.\n",
    "is_tour_window: The coefficient is 1.2541, but with a p-value of 0.282, it's not statistically significant at the 0.05 level. This means being in a tour window does not have a statistically significant effect on the adjusted elapsed time in this model.\n",
    "weather_delay: The coefficient is -0.0513, also not statistically significant (p = 0.377), indicating that weather delays do not have a meaningful impact on elapsed time in this context.\n",
    "distance: The coefficient of 0.1135 is highly significant (p < 0.001), meaning that for every unit increase in distance, the adjusted elapsed time increases significantly. This aligns with intuitive expectations that longer flights take more time.\n",
    "\n",
    "Statistical Significance:\n",
    "\n",
    "Only distance shows strong significance, while the other predictors do not.\n",
    "Model Fit and Assumptions:\n",
    "\n",
    "The F-statistic is very high (2659) with a p-value of 0.00, indicating that at least one predictor is significantly related to the response variable.\n",
    "Normality of Residuals: The Omnibus test, Jarque-Bera test, and the skewness indicate possible deviations from normality. This could suggest a need to inspect residuals further for patterns or outliers.\n",
    "Durbin-Watson statistic: Close to 2 indicates little autocorrelation in residuals, which is a good sign.\n",
    "\n",
    "Multicollinearity:\n",
    "\n",
    "The warning about a large condition number (2.70e+03) suggests potential multicollinearity among your predictors. Check the Variance Inflation Factor (VIF) for your predictors to assess multicollinearity:\n",
    "\n",
    "Address Multicollinearity: Check the correlation matrix of your predictors and use Variance Inflation Factor (VIF) to identify problematic variables.\n",
    "Transform the Target Variable: Consider applying a log transformation to delay_in_min to make the model more interpretable and potentially improve the fit.\n",
    "Model Refinement: Try different models that are more robust to non-normality and autocorrelation, such as Ridge or Lasso regression, and assess whether they improve model performance.\n",
    "Residual Diagnostics: Plot the residuals to better understand the issues with autocorrelation and non-normality, and potentially apply time-series techniques if the data is time-dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18a80b-75ef-4901-9108-6c0abbaf975e",
   "metadata": {},
   "source": [
    "Q-Q Plot: Check how closely the points follow the diagonal line. Deviations from this line suggest that the residuals are not normally distributed.\n",
    "Next Steps non-normality, consider transformations of the dependent variable or adding interaction terms or polynomial features. You can also apply statistical tests for normality (like the Shapiro-Wilk test) or homoscedasticity (like Breusch-Pagan test) for a more formal assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5deccf7-d22f-4f28-a34e-0f94b1004ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numeric using label encoding (if applicable)\n",
    "df['eras_tour'] = df['eras_tour'].astype('category').cat.codes\n",
    "df['nfl_game'] = df['nfl_game'].astype('category').cat.codes\n",
    "df['day_of_week'] = df['day_of_week'].astype('category').cat.codes\n",
    "df['origin_state'] = df['origin_state'].astype('category').cat.codes\n",
    "df['dest_state'] = df['dest_state'].astype('category').cat.codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4329c-7455-40e2-8bbc-82ac0c8e797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "correlation_matrix = df[['delay_in_min', 'eras_tour',  'nfl_game', 'day_of_week', 'origin_state', 'dest_state' ]].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64ff64-ad3f-40ec-ab53-ab0a4c8df822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331869a-fe33-4d1f-88f8-fe751a79bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delays by days of state\n",
    "\n",
    "# # Delays by days of week\n",
    "# sns.heatmap(x='day_of_week', y='origin_state', data=df)\n",
    "# plt.title('Flight Delays by Day by State')\n",
    "# plt.xlabel('State')\n",
    "# plt.ylabel('Delay (minutes)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bada4-b886-4049-968e-6945181c862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of delays greater than 0 by day_of_week\n",
    "delay_percentage = df.groupby('day_of_week')['delay_in_min'].apply(lambda x: (x > 0).mean() * 100)\n",
    "\n",
    "# # Plot the result\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x=delay_percentage.index, y=delay_percentage.values, palette='viridis')\n",
    "# plt.title('Percentage of Flights with Delay > 0 by Day of the Week')\n",
    "# plt.xlabel('Day of the Week')\n",
    "# plt.ylabel('Percentage of Delayed Flights (%)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c4d08-0492-4547-a426-a66a331763e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flight delays by state\n",
    "# avg_delay_by_state = df.groupby('origin_state')['delay_in_min'].mean().reset_index()\n",
    "\n",
    "# # Create the bar plot\n",
    "# sns.barplot(x='dest_state', y='delay', data=avg_delay_by_state)\n",
    "# plt.title('Average Flight Delay by State')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.ylabel('Average Delay (minutes)')\n",
    "# plt.xlabel('Origin State')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089416a-71f7-41f9-9a90-5b11b12681c6",
   "metadata": {},
   "source": [
    "Selection deleted\n",
    "\n",
    "Some seasonal correlations are slightly positive (e.g., Summer Arrival Delay: 0.042, Winter Departure Delay: 0.009668), but these values are still weak.\n",
    "\n",
    "Similar to latitude, longitude also shows low correlation with delays. Most values are close to zero, indicating no significant relationship.\n",
    "\n",
    "Explore other factors that might impact delays, such as weather conditions (precipitation, snow), airport traffic, or operational factors (e.g., carrier delays).\n",
    "\n",
    "Consider using statistical models (like linear regression) to analyze the influence of various factors, including latitude, longitude, and weather, on flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f115d97-9d6c-4a98-a06e-cc691054b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison Table\n",
    "\n",
    "# Convert results to DataFrame\n",
    "performance_df = pd.DataFrame(results).T\n",
    "print(performance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f640505-5bf0-425a-b967-8d0ca6a7ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal_delays = df.groupby('season')[['delay_in_min']].mean()\n",
    "# print(seasonal_delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4f596-7198-41df-8304-681d55cd9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider seasons\n",
    "\n",
    "# df['season'] = df['date'].dt.month % 12 // 3 + 1\n",
    "# # Mapping months to seasons\n",
    "# season_mapping = {\n",
    "#     1: 'winter',\n",
    "#     2: 'spring',\n",
    "#     # 3: 'summer',\n",
    "#     4: 'fall'\n",
    "# }\n",
    "# df['season'] = df['season'].map(season_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6489e3-96bb-4880-bd51-7f19f25df5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# # Define a function to get the season based on the month\n",
    "# def get_season(date):\n",
    "#     month = date.month\n",
    "#     if month in [12, 1, 2]:\n",
    "#         return 'winter'\n",
    "#     # elif month in [3, 4, 5]:\n",
    "#         return 'spring'\n",
    "#     elif month in [6, 7, 8]:\n",
    "#         return 'summer'\n",
    "#     else:\n",
    "#         return 'fall'\n",
    "\n",
    "# # Apply the function to create the season column\n",
    "# df['season'] = df['date'].apply(get_season)\n",
    "\n",
    "# # Now sample the data again\n",
    "# sampled_data = df.sample(frac=0.1, random_state=1)\n",
    "\n",
    "# # Scatter plot for Arrival Delay vs Latitude (using sampled data)\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.scatterplot(data=sampled_data, x='dest_latitude', y='arrival_delay', hue='season', alpha=0.7)\n",
    "# plt.title('Arrival Delay vs Latitude by Season (Sampled)')\n",
    "# plt.axhline(0, color='red', linestyle='--')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Arrival Delay (minutes)')\n",
    "\n",
    "# # Remove legend\n",
    "# plt.legend([], [], frameon=False)\n",
    "\n",
    "# # Scatter plot for Departure Delay vs Latitude (using sampled data)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.scatterplot(data=sampled_data, x='origin_latitude', y='departure_delay', hue='season', alpha=0.7)\n",
    "# plt.title('Departure Delay vs Latitude by Season (Sampled)')\n",
    "# plt.axhline(0, color='red', linestyle='--')\n",
    "# plt.xlabel('Latitude')\n",
    "# plt.ylabel('Departure Delay (minutes)')\n",
    "\n",
    "# # Remove legend\n",
    "# plt.legend([], [], frameon=False)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb167a-442a-4c86-8551-439240beedcc",
   "metadata": {},
   "source": [
    "Compare the seasonal delays with other factors such as weather conditions or day of the week to identify any correlations.\n",
    "Investigate if the delays are statistically significant between seasons using ANOVA or similar statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a2be6-a0e0-45de-b705-f0ea8c6c8f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal_delays.plot(kind='bar', figsize=(10, 6))\n",
    "# plt.title('Average Arrival and Departure Delays by Season')\n",
    "# plt.ylabel('Delay (minutes)')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.axhline(0, color='red', linestyle='--')\n",
    "# plt.legend(title='Delay Type')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00010136-db3b-4d37-82d8-24290530181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot delays vs elevation\n",
    "# def plot_delay_vs_elevation(data):\n",
    "#     plt.figure(figsize=(14, 6))\n",
    "\n",
    "#     # Arrival Delay\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     sns.scatterplot(data=data, x='elevation', y='arrival_delay', hue='season', alpha=0.7)\n",
    "#     plt.title('Arrival Delay vs Elevation by Season')\n",
    "#     plt.axhline(0, color='red', linestyle='--')\n",
    "#     plt.xlabel('Elevation (feet)')\n",
    "#     plt.ylabel('Arrival Delay (minutes)')\n",
    "\n",
    "#     # Departure Delay\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     sns.scatterplot(data=data, x='elevation', y='departure_delay', hue='season', alpha=0.7)\n",
    "#     plt.title('Departure Delay vs Elevation by Season')\n",
    "#     plt.axhline(0, color='red', linestyle='--')\n",
    "#     plt.xlabel('Elevation (feet)')\n",
    "#     plt.ylabel('Departure Delay (minutes)')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the plotting function with the correct DataFrame\n",
    "# plot_delay_vs_elevation(flights_weather_df)\n",
    "\n",
    "# # Calculate Correlation Coefficients\n",
    "# elevation_delay_corr = flights_weather_df.groupby('season')[['elevation', 'arrival_delay', 'departure_delay']].corr()\n",
    "# elevation_delay_corr = elevation_delay_corr.reset_index()\n",
    "\n",
    "# # Extracting relevant correlation data\n",
    "# arrival_elevation_corr = elevation_delay_corr[elevation_delay_corr['level_1'] == 'arrival_delay']\n",
    "# departure_elevation_corr = elevation_delay_corr[elevation_delay_corr['level_1'] == 'departure_delay']\n",
    "\n",
    "# # Print correlation coefficients\n",
    "# print(\"Arrival Delay Correlation with Elevation:\")\n",
    "# print(arrival_elevation_corr[['season', 'elevation', 'arrival_delay', 'level_1']])\n",
    "# print(\"\\nDeparture Delay Correlation with Elevation:\")\n",
    "# print(departure_elevation_corr[['season', 'elevation', 'departure_delay', 'level_1']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3663ec-d816-4cb8-9648-78aa16d7a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  # SVC for classification, SVR for regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007d8e2-77f1-4b0d-8e17-795e3e895b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the Support Vector Regression model\n",
    "svm_model = SVR(kernel='rbf', C=1e3, gamma=0.1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cf779-f8f1-4364-8dcd-5ea0bfbbfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341cc1-b0c7-4d2b-8aa0-0cfb69e5be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef8bcc-e180-49a0-8a25-6389d15549cb",
   "metadata": {},
   "source": [
    "If the target variable is intentionally constant (which would be an unusual case), would need to reconsider the problem we're solving, as predictive models typically require variance in the target variable to learn meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53794a01-e1ac-45bf-b69e-a54bcac67d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the class distribution of y_train\n",
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e63ef6-2840-40bb-9ad5-dc7e16cd2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train Distribution:\", y_train.value_counts())\n",
    "print(\"y_test Distribution:\", y_test.value_counts())\n",
    "\n",
    "\n",
    "# Split the reduced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced_2d, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be9738-5a06-44ce-abb1-0faece941fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the Support Vector Regressor model\n",
    "svm_regressor = SVR()\n",
    "\n",
    "# Fit the model\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svr = svm_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model (regression: RMSE)\n",
    "rmse_svr = mean_squared_error(y_test, y_pred_svr, squared=False)\n",
    "\n",
    "# Print the RMSE for the Support Vector Regressor model\n",
    "print(f\"SVM Regressor RMSE: {rmse_svr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701425c6-2ed8-4711-a3cd-4065bf613cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train Distribution:\", y_train.value_counts())\n",
    "print(\"y_test Distribution:\", y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6105cf-0566-4590-b9f8-03947213de0e",
   "metadata": {},
   "source": [
    "Final Evaluation\n",
    "After handling class imbalance, rerun the evaluation metrics, including:\n",
    "\n",
    "Confusion Matrix: To checked whether the model is predicting both classes.\n",
    "Precision, Recall, F1 Score: To assess the model's performance.\n",
    "ROC-AUC: To evaluate the model's ability to discriminate between classes.\n",
    "\n",
    "\n",
    "\n",
    "After performing all these tests, the results have come up inconclusive with testing due to the class imbalance. I will time cleaning and evaluating the data so that the scoring continues to incprove and make sure we have enough balance between classes because there are too many zeros in the current dataset. \n",
    "\n",
    "Will perform additional testing with removing all data with no weather data to because I expect that to increase modeling performance. We opted to keep these features in the model at first because if some of the missing data was only missing because it wasn't reported (not necessarily zero values), we wanted to be able to explore that further to enhance model performance and experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
