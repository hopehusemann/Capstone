{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "e712af2e-c077-42ba-8afc-598578e1da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "937f2b71-9c1e-4405-93ab-4a5e2d402b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[387], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m flights_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhopeh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata_science_bootcamp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mflight_times_capstone\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcleaned_combined_data_v2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "flights_df = pd.read_csv(r\"C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\cleaned_combined_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4448423-e084-417e-b502-856d3b684f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tswift_df = pd.read_csv(r\"C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\extra_fun_data\\Taylor_Swift_Eras_Tour.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1321705-05f1-443e-87f3-11eb4f84bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tswift_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e586d9-e337-4ca6-bc71-7e806bc229e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_columns = tswift_df.columns[tswift_df.isna().all()].tolist()\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e799f-19e3-4856-8e49-75597d89c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are 100% NaN\n",
    "tswift_df = tswift_df.dropna(axis=1, how='all')\n",
    "\n",
    "# Drop rows that are 100% NaN\n",
    "tswift_df = tswift_df.dropna(axis=0, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bddd0-598f-4881-bab1-3794fed8c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_rows = tswift_df[tswift_df['Unnamed: 1'].notna()]\n",
    "print(non_nan_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685bea09-4c06-465d-8d35-6e2c39e9c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tswift_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4a107-033b-46d4-b8cd-a318c768ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the first two rows using iloc\n",
    "tswift_df = tswift_df.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e5a25-c57d-466f-a03b-200494010d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tswift_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2e7be-2ba0-4baa-a22e-f731bee04418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = ['tour_date', 'tour_city', 'tour_stadium', 'tour_opener', 'notes']\n",
    "\n",
    "# Replace the column names for the specified columns\n",
    "tswift_df.columns = new_column_names + list(tswift_df.columns[len(new_column_names):])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28510bda-3114-42e0-976f-21a7fcb7e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Tour Dates' column has NaN values\n",
    "tswift_df = tswift_df.dropna(subset=['tour_date'])\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "tswift_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df873e5-a9ee-4189-a891-de57c9adb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Tour Dates' column\n",
    "tswift_df = tswift_df.drop(columns=['tour_date'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658166b-c5fc-4319-ac06-49fb1cd65b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = ['tour_date', 'tour_city', 'tour_stadium', 'tour_opener', 'notes']\n",
    "\n",
    "# Replace the column names for the specified columns\n",
    "tswift_df.columns = new_column_names + list(tswift_df.columns[len(new_column_names):])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf63c3-9906-40ab-8b78-7fc188af3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "tswift_df = tswift_df.drop(columns=['notes', 'Unnamed: 7', 'Unnamed: 11'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb3d37-d83c-4068-8470-4859087905e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'tour_date' is in datetime format for proper sorting\n",
    "tswift_df['tour_date'] = pd.to_datetime(tswift_df['tour_date'])\n",
    "\n",
    "# Sort the DataFrame by 'tour_date'\n",
    "tswift_df = tswift_df.sort_values(by='tour_date').reset_index(drop=True)\n",
    "\n",
    "# Fill NaN values in 'tour_city' with the nearest city name above it\n",
    "tswift_df['tour_city'] = tswift_df['tour_city'].fillna(method='ffill')\n",
    "tswift_df['tour_stadium'] = tswift_df['tour_stadium'].fillna(method='ffill')\n",
    "# Display the updated DataFrame\n",
    "print(tswift_df.head(-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eae51-4749-4636-92df-970792f01ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'tour_date' is in datetime format\n",
    "tswift_df['tour_date'] = pd.to_datetime(tswift_df['tour_date'])\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2023-05-01'\n",
    "end_date = '2024-04-30'\n",
    "\n",
    "# Filter the DataFrame for dates within the specified range\n",
    "filtered_df = tswift_df[(tswift_df['tour_date'] >= start_date) & (tswift_df['tour_date'] <= end_date)]\n",
    "\n",
    "# Reset the index for the filtered DataFrame\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be75cd-bc2c-4212-b4d5-fc9ee61d427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_tour = {\n",
    "    \"Date\": [\n",
    "        \"March 17\", \"March 18\", \"March 24\", \"March 25\", \"March 31\", \n",
    "        \"April 1\", \"April 2\", \"April 13\", \"April 14\", \"April 15\", \n",
    "        \"April 21\", \"April 22\", \"April 23\", \"April 28\", \"April 29\", \n",
    "        \"April 30\", \"May 5\", \"May 6\", \"May 7\", \"May 12\", \n",
    "        \"May 13\", \"May 14\", \"May 19\", \"May 20\", \"May 21\", \n",
    "        \"May 26\", \"May 27\", \"May 28\", \"June 2\", \"June 3\", \n",
    "        \"June 4\", \"June 9\", \"June 10\", \"June 16\", \"June 17\", \n",
    "        \"June 23\", \"June 24\", \"June 30\", \"July 1\", \"July 7\", \n",
    "        \"July 8\", \"July 14\", \"July 15\", \"July 22\", \"July 23\", \n",
    "        \"July 28\", \"July 29\", \"August 3\", \"August 4\", \"August 5\", \n",
    "        \"August 7\", \"August 8\", \"August 9\"\n",
    "\n",
    "    ],\n",
    "    \"City\": [\n",
    "        \"Glendale\", \"Glendale\", \"Paradise\", \"Paradise\", \"Arlington\", \n",
    "        \"Arlington\", \"Arlington\", \"Tampa\", \"Tampa\", \"Tampa\", \n",
    "        \"Houston\", \"Houston\", \"Houston\", \"Atlanta\", \"Atlanta\", \n",
    "        \"Atlanta\", \"Nashville\", \"Nashville\", \"Nashville\", \"Philadelphia\", \n",
    "        \"Philadelphia\", \"Philadelphia\", \"Foxborough\", \"Foxborough\", \n",
    "        \"Foxborough\", \"East Rutherford\", \"East Rutherford\", \n",
    "        \"East Rutherford\", \"Chicago\", \"Chicago\", \"Chicago\", \n",
    "        \"Detroit\", \"Detroit\", \"Pittsburgh\", \"Pittsburgh\", \"Minneapolis\", \n",
    "        \"Minneapolis\", \"Cincinnati\", \"Cincinnati\", \"Kansas City\", \n",
    "        \"Kansas City\", \"Denver\", \"Denver\", \"Seattle\", \"Seattle\", \n",
    "        \"Santa Clara\", \"Santa Clara\", \"Inglewood\", \"Inglewood\", \n",
    "        \"Inglewood\", \"Inglewood\", \"Inglewood\", \"Inglewood\", \n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012441be-47d9-4e72-83ce-efc264b42958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lengths of lists in eras_tour\n",
    "print(\"Lengths of eras_tour columns:\")\n",
    "for key, value in eras_tour.items():\n",
    "    print(f\"{key}: {len(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37c608-63c8-43a7-bd76-59bfa1cc5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating DataFrames\n",
    "eras_tour = pd.DataFrame(eras_tour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d37b3e-7dc4-471a-89c0-c26212ea7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_tour.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab6c9e-5ab7-4556-a222-1c7917a0d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of cities to IATA codes for the nearest large airports\n",
    "iata_mapping = {\n",
    "    \"Glendale\": \"PHX\",          # Phoenix Sky Harbor International Airport\n",
    "    \"Paradise\": \"LAS\",          # Harry Reid International Airport (Las Vegas)\n",
    "    \"Arlington\": \"DFW\",         # Dallas/Fort Worth International Airport\n",
    "    \"Tampa\": \"TPA\",             # Tampa International Airport\n",
    "    \"Houston\": \"IAH\",           # George Bush Intercontinental Airport\n",
    "    \"Atlanta\": \"ATL\",           # Hartsfield-Jackson Atlanta International Airport\n",
    "    \"Nashville\": \"BNA\",         # Nashville International Airport\n",
    "    \"Philadelphia\": \"PHL\",      # Philadelphia International Airport\n",
    "    \"Foxborough\": \"BOS\",        # Logan International Airport (Boston)\n",
    "    \"East Rutherford\": \"EWR\",   # Newark Liberty International Airport\n",
    "    \"Chicago\": \"ORD\",           # O'Hare International Airport\n",
    "    \"Detroit\": \"DTW\",           # Detroit Metropolitan Airport\n",
    "    \"Pittsburgh\": \"PIT\",        # Pittsburgh International Airport\n",
    "    \"Minneapolis\": \"MSP\",       # Minneapolis–Saint Paul International Airport\n",
    "    \"Cincinnati\": \"CVG\",        # Cincinnati/Northern Kentucky International Airport\n",
    "    \"Kansas City\": \"MCI\",       # Kansas City International Airport\n",
    "    \"Denver\": \"DEN\",            # Denver International Airport\n",
    "    \"Seattle\": \"SEA\",           # Seattle–Tacoma International Airport\n",
    "    \"Santa Clara\": \"SJC\",       # San Jose International Airport\n",
    "    \"Inglewood\": \"LAX\",         # Los Angeles International Airport\n",
    "    \"Mexico City\": \"MEX\",       # Mexico City International Airport\n",
    "    \"Buenos Aires\": \"EZE\",      # Ministro Pistarini International Airport\n",
    "    \"Rio de Janeiro\": \"GIG\",    # Rio de Janeiro/Galeão International Airport\n",
    "    \"São Paulo\": \"GRU\",         # São Paulo/Guarulhos–Governador André Franco Montoro International Airport\n",
    "    \"Tokyo\": \"NRT\",             # Narita International Airport\n",
    "    \"Melbourne\": \"MEL\",         # Melbourne Airport\n",
    "    \"Sydney\": \"SYD\",            # Sydney Kingsford Smith Airport\n",
    "    \"Singapore\": \"SIN\",         # Singapore Changi Airport\n",
    "}\n",
    "\n",
    "# Use this mapping to create a new column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2871fc-4435-4d6b-a96a-690fe6a3517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map city to IATA code\n",
    "def get_iata_code(city):\n",
    "    return iata_mapping.get(city, None)\n",
    "\n",
    "# Create a new column for IATA codes\n",
    "eras_tour['IATA'] = eras_tour['City'].apply(get_iata_code)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(eras_tour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a026e69-189b-44c4-9321-3e3d255ae757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the 'Date' column to datetime format using the year 2023\n",
    "eras_tour['Date'] = pd.to_datetime(eras_tour['Date'] + ' 2023', format='%B %d %Y')\n",
    "\n",
    "# Review the updated DataFrame\n",
    "print(eras_tour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30b304-d935-4124-a525-428196ba1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_tour['eras_tour'] = \"Tour Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9cf02-0560-4197-9a41-8efd3f79b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea7087-d358-485a-9136-5ad9799fbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the eras_tour DataFrame to a CSV file\n",
    "eras_tour.to_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\eras_tour_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f5359-514b-44cf-809b-1c25a275bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "eras_tour_df = pd.read_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\eras_tour_data.csv')\n",
    "combined_data = pd.read_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\cleaned_combined_data_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a1dcf-2698-49cd-b5c4-25ce08851bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both date columns are in the same format (if necessary)\n",
    "combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "eras_tour_df['Date'] = pd.to_datetime(eras_tour_df['Date'])\n",
    "\n",
    "print(combined_data['date'].unique())\n",
    "print(eras_tour_df['Date'].unique())\n",
    "print(combined_data['origin_iata'].unique())\n",
    "print(combined_data['dest_iata'].unique())\n",
    "print(eras_tour_df['IATA'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46612b1-4310-42ab-88c3-82de61a5854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize datasets, applying to strings only\n",
    "eras_tour_df = eras_tour_df.applymap(lambda x: x.strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Check the changes\n",
    "print(eras_tour_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6c7ee-34a8-40f4-b823-baaf5266665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Perform a merge to find matches\n",
    "merged_data = pd.merge(combined_data, eras_tour_df, how='left', left_on=['date', 'origin_iata'], right_on=['Date', 'IATA'])\n",
    "merged_data = pd.merge(merged_data, eras_tour_df, how='left', left_on=['date', 'dest_iata'], right_on=['Date', 'IATA'], suffixes=('', '_dest'))\n",
    "\n",
    "# Update 'era_tour' based on the matches found\n",
    "combined_data['eras_tour'] = merged_data.apply(lambda x: 'tour_date' if x['Date'] is not pd.NaT else '', axis=1)\n",
    "\n",
    "# Drop any unnecessary columns if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d108e9-c36e-4573-a3f8-bf7e28b63fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where 'tour_date' is not empty\n",
    "tour_date_count = combined_data['eras_tour'].value_counts().get('tour_date', 0)\n",
    "\n",
    "print(f\"Number of rows with 'tour_date' set: {tour_date_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f528ccc-9be2-4cf8-b75d-50ccc1b3e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910f92c-62ba-4698-8dbb-5211cde058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE TO CSV \n",
    "combined_data.to_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\all_combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0e565-895b-4343-8b0b-89494b6bf9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31470545-3b2b-4cdd-8152-8cdd1377765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddf88e-1863-479d-ae03-bf996d6fd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d3a7a-cad7-48f0-b203-6778de824e4d",
   "metadata": {},
   "source": [
    "Key Columns for Analysis\n",
    "\n",
    "Date and Time Columns:\n",
    "\n",
    "date: The date of the flight, useful for temporal analysis.\n",
    "day_of_week: To analyze patterns based on the day of the week.\n",
    "Delay Information:\n",
    "\n",
    "total_delay_time: Total delay time for the flight, essential for understanding delay patterns.\n",
    "\n",
    "arrival_delay and departure_delay: Specific arrival and departure delays for deeper insights.\n",
    "\n",
    "Delay reasons: carrier_delay, weather_delay, nas_delay, security_delay, late_aircraft_delay can provide insight into what might be causing delays on tour dates.\n",
    "\n",
    "    \n",
    "Flight Characteristics:\n",
    "\n",
    "op_unique_carrier: Carrier information, which can help determine if certain airlines were more affected.\n",
    "origin_iata and dest_iata: To see if certain routes are more impacted.\n",
    "\n",
    "Geospatial Information:\n",
    "\n",
    "origin_latitude, origin_longitude, dest_latitude, dest_longitude: Useful for visualizing delays geographically.\n",
    "origin_city and dest_city: Helpful for identifying specific cities involved in the analysis.\n",
    "\n",
    "\n",
    "Weather Information:\n",
    "\n",
    "prcp, snow, snwd, tmax, tmin: Weather data can provide context on whether delays were influenced by weather conditions.\n",
    "Tour Indicators:\n",
    "\n",
    "tour_date: Indicates if the flight is on a tour date.\n",
    "\n",
    "eras_tour: If this column is set, it can signify the specific tour event impacting the flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8a645-c49e-4382-8fe5-e02013ab48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold unique columns\n",
    "unique_columns = []\n",
    "\n",
    "# Set to track seen columns\n",
    "seen = set()\n",
    "\n",
    "# Process columns in chunks\n",
    "chunk_size = 100  # Adjust this size based on your memory and performance\n",
    "for start in range(0, combined_data.shape[1], chunk_size):\n",
    "    end = min(start + chunk_size, combined_data.shape[1])\n",
    "    chunk = combined_data.iloc[:, start:end]\n",
    "    \n",
    "    # Identify duplicates in the current chunk\n",
    "    for col in chunk.columns:\n",
    "        if col not in seen:\n",
    "            unique_columns.append(col)\n",
    "            seen.add(col)\n",
    "\n",
    "# Create a new DataFrame with only unique columns\n",
    "combined_data_cleaned = combined_data[unique_columns]\n",
    "\n",
    "# Display the cleaned DataFrame's columns\n",
    "print(\"Columns after removing duplicate columns:\")\n",
    "print(combined_data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634cf50-0140-4465-a1f0-e62df40d62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65932d-b02e-452f-a955-2e023505f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data_cleaned.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af2203-cd20-454f-8f2f-e921d7bc0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE TO CSV \n",
    "combined_data_cleaned.to_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\all_combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de40d73-9b6b-48de-9301-2d103b98537e",
   "metadata": {},
   "source": [
    "Flight Count on Tour Dates vs. Non-Tour Dates:\n",
    "\n",
    "Analyze how many flights occurred on tour dates compared to non-tour dates.\n",
    "Delay Patterns:\n",
    "\n",
    "Calculate average, median, and total delays for flights on tour dates versus non-tour dates.\n",
    "Visualize the distribution of delays with box plots.\n",
    "Impact by Airline:\n",
    "\n",
    "Investigate if certain carriers experienced more delays on tour dates.\n",
    "Weather Analysis:\n",
    "\n",
    "Examine whether weather conditions during tour dates contributed to delays.\n",
    "Time Series Analysis:\n",
    "\n",
    "Look at how delays varied over time, especially around tour dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11db0c-0576-4402-b278-6f6d05edccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of flights on tour dates vs non-tour dates\n",
    "flight_counts = combined_data_cleaned.groupby('eras_tour')['op_unique_carrier'].count()\n",
    "print(flight_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655855c8-484c-44d7-aadf-5e56482829dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total_delay_time\n",
    "def calculate_total_delay(crs_arr_time, arr_time):\n",
    "    delay = crs_arr_time - arr_time\n",
    "    if arr_time < -4:\n",
    "        return 'early'\n",
    "    elif arr_time >= -4 and arr_time < 0:\n",
    "        return 'on_time'\n",
    "    elif arr_time >= 0:\n",
    "        return delay\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the new column\n",
    "combined_data_cleaned['total_delay_time'] = combined_data_cleaned.apply(lambda row: calculate_total_delay(row['crs_arr_time'], row['arr_time']), axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(combined_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fd825-f522-4edf-936e-abc3cffa5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_cleaned.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c26484-684b-477a-8562-1ac01893973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for delays\n",
    "delay_stats = combined_data_cleaned.groupby('eras_tour')['total_delay_time'].describe()\n",
    "print(delay_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a96f6b-9fd4-4db1-b269-03b08cad7adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization of delay distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='eras_tour', y='total_delay_time', data=combined_data_cleaned)\n",
    "plt.xticks([0, 1], ['Non-Tour Dates', 'Tour Dates'])\n",
    "plt.title('Flight Delays on Tour Dates vs Non-Tour Dates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0642a3-756b-4b27-a259-f2375635f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis of average delays\n",
    "avg_delay_time = combined_data_cleaned.groupby('date')['total_delay_time'].mean()\n",
    "avg_delay_time.plot(figsize=(12, 6), title='Average Flight Delay Over Time')\n",
    "plt.axvline(x='2023-05-05', color='red', linestyle='--')  # Example tour date\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dbc9f-ec53-40e6-b335-83d944b2c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Negative Delays to Zero\n",
    "# \n",
    "combined_data_cleaned['adjusted_delay_time'] = combined_data_cleaned['total_delay_time'].clip(lower=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51bc1d-cbe1-467a-a8d2-e2580f1b1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean the 'eras_tour' column\n",
    "combined_data_cleaned['eras_tour'] = combined_data_cleaned['eras_tour'].replace({'tour_date': 1, '': 0})\n",
    "\n",
    "# Step 2: Verify the unique values again\n",
    "print(combined_data_cleaned['eras_tour'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c92e7d-f9c6-4431-b012-fad9cfa30fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define tour and non-tour dates\n",
    "tour_flights = combined_data_cleaned[combined_data_cleaned['eras_tour'] == 1]\n",
    "non_tour_flights = combined_data_cleaned[combined_data_cleaned['eras_tour'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2a411-c304-4d51-9794-7f6a4bd8922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of filtered DataFrames\n",
    "print(f'Tour flights shape: {tour_flights.shape}')\n",
    "print(f'Non-tour flights shape: {non_tour_flights.shape}')\n",
    "\n",
    "# Ensure there is data before proceeding with calculations\n",
    "if not tour_flights.empty and not non_tour_flights.empty:\n",
    "    # Step 4: Adjust delays to be non-negative for both DataFrames\n",
    "    tour_flights['adjusted_elapsed_time'] = tour_flights['actual_elapsed_time'].clip(lower=0)\n",
    "    non_tour_flights['adjusted_elapsed_time'] = non_tour_flights['actual_elapsed_time'].clip(lower=0)\n",
    "\n",
    "    # Calculate delay metrics\n",
    "    delay_metrics = {\n",
    "        'Tour Dates': {\n",
    "            'Average Delay': tour_flights['adjusted_elapsed_time'].mean(),\n",
    "            'Median Delay': tour_flights['adjusted_elapsed_time'].median(),\n",
    "            'Total Delay': tour_flights['adjusted_elapsed_time'].sum()\n",
    "        },\n",
    "        'Non-Tour Dates': {\n",
    "            'Average Delay': non_tour_flights['adjusted_elapsed_time'].mean(),\n",
    "            'Median Delay': non_tour_flights['adjusted_elapsed_time'].median(),\n",
    "            'Total Delay': non_tour_flights['adjusted_elapsed_time'].sum()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Display delay metrics\n",
    "    delay_df = pd.DataFrame(delay_metrics).T\n",
    "    print(delay_df)\n",
    "else:\n",
    "    print(\"No data found for either tour or non-tour dates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be9885-25b7-4876-9d0d-21f47a3b1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# A\n",
    "# Display delay metrics\n",
    "delay_df = pd.DataFrame(delay_metrics).T\n",
    "print(delay_df)\n",
    "\n",
    "# Step 4: Visualize delay distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=[tour_flights['adjusted_elapsed_time'], non_tour_flights['adjusted_elapsed_time']], \n",
    "            palette=[\"lightblue\", \"lightgreen\"])\n",
    "plt.xticks([0, 1], ['Tour Dates', 'Non-Tour Dates'])\n",
    "plt.title('Distribution of Adjusted Actual Elapsed Time')\n",
    "plt.ylabel('Adjusted Actual Elapsed Time (in minutes)')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Analyze impact by airline\n",
    "airline_delays = combined_data_cleaned.groupby(['op_unique_carrier', 'eras_tour'])['adjusted_elapsed_time'].agg(['mean', 'median', 'sum']).reset_index()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=airline_delays, x='op_unique_carrier', y='mean', hue='eras_tour', palette='pastel')\n",
    "plt.title('Average Adjusted Delay by Airline on Tour vs Non-Tour Dates')\n",
    "plt.ylabel('Average Adjusted Delay (in minutes)')\n",
    "plt.xlabel('Airline Carrier')\n",
    "plt.legend(title='Tour Date', labels=['Non-Tour', 'Tour'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Weather Analysis\n",
    "# Assuming `weather_delay` indicates weather-related delays\n",
    "weather_impact = combined_data_cleaned.groupby('eras_tour')['weather_delay'].agg(['mean', 'median', 'sum']).reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=weather_impact, x='eras_tour', y='mean', palette='coolwarm')\n",
    "plt.title('Average Weather Delay on Tour vs Non-Tour Dates')\n",
    "plt.ylabel('Average Weather Delay (in minutes)')\n",
    "plt.xticks(ticks=[0, 1], labels=['Non-Tour', 'Tour'])\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Time Series Analysis\n",
    "# Convert 'date' to datetime if not already\n",
    "combined_data_cleaned['date'] = pd.to_datetime(combined_data_cleaned['date'])\n",
    "\n",
    "# Plot adjusted delays over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=combined_data_cleaned, x='date', y='adjusted_elapsed_time', hue='eras_tour', estimator='mean', ci=None)\n",
    "plt.title('Average Adjusted Delay Over Time by Tour Dates')\n",
    "plt.ylabel('Average Adjusted Delay (in minutes)')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Tour Date', labels=['Non-Tour', 'Tour'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fc0fd-8129-4818-aff4-c3f3012aeb5a",
   "metadata": {},
   "source": [
    "The standard deviations are relatively high, suggesting considerable variability in delay times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0fc26-b070-4e8b-bfe2-7495ec1e69d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tour_flights.shape)\n",
    "print(non_tour_flights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa9db8-f11a-4b4c-bd79-806e0bab6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the date column is in datetime format using .loc to avoid SettingWithCopyWarning\n",
    "non_tour_flights.loc[:, 'date'] = pd.to_datetime(non_tour_flights['date'])\n",
    "\n",
    "# Find the earliest tour date\n",
    "earliest_tour_date = non_tour_flights[non_tour_flights['eras_tour'] == 'tour_date']['date'].min()\n",
    "\n",
    "# Calculate cutoff date (one month before the earliest tour date)\n",
    "cutoff_date = earliest_tour_date - pd.DateOffset(months=1)\n",
    "\n",
    "# Filter the non_tour_flights dataset\n",
    "filtered_non_tour_flights = non_tour_flights[non_tour_flights['date'] >= cutoff_date]\n",
    "\n",
    "# Display the filtered dataset\n",
    "print(filtered_non_tour_flights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd60aa2-3c5f-472e-9594-b7d20ca49a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Earliest Tour Date:\", earliest_tour_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce9ce5-5148-4776-b930-84653b485ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data_cleaned['eras_tour'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83e64b-b0d2-4174-a71c-c0da7afc1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "non_tour_flights.loc[:, 'date'] = pd.to_datetime(non_tour_flights['date'])\n",
    "\n",
    "# Find the earliest tour date where eras_tour == 1\n",
    "earliest_tour_date = non_tour_flights[non_tour_flights['eras_tour'] == 0]['date'].min()\n",
    "\n",
    "# Check if there's a valid earliest tour date\n",
    "if pd.isna(earliest_tour_date):\n",
    "    print(\"No tour dates found in eras_tour.\")\n",
    "else:\n",
    "    print(\"Earliest Tour Date:\", earliest_tour_date)\n",
    "\n",
    "    # Calculate cutoff date (one month before the earliest tour date)\n",
    "    cutoff_date = earliest_tour_date - pd.DateOffset(months=1)\n",
    "    print(\"Cutoff Date:\", cutoff_date)\n",
    "\n",
    "    # Check min and max dates in the dataset\n",
    "    print(\"Min Date:\", non_tour_flights['date'].min())\n",
    "    print(\"Max Date:\", non_tour_flights['date'].max())\n",
    "\n",
    "    # Filter the non_tour_flights dataset\n",
    "    filtered_non_tour_flights = non_tour_flights[non_tour_flights['date'] >= cutoff_date]\n",
    "\n",
    "    # Display the filtered dataset\n",
    "    print(filtered_non_tour_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d570b-af26-41a7-8553-e367dc0898d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the non_tour_flights dataset for dates after the cutoff\n",
    "filtered_non_tour_flights = non_tour_flights[non_tour_flights['date'] >= cutoff_date]\n",
    "\n",
    "# Display the filtered dataset\n",
    "print(\"Filtered Non-Tour Flights:\")\n",
    "print(filtered_non_tour_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c997031-b492-4ee7-8939-44661a9988a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of records in the filtered dataset\n",
    "print(\"Number of records after filtering:\", filtered_non_tour_flights.shape[0])\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(filtered_non_tour_flights.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d726e7-d9c9-4d68-84ae-fd2e639312cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_non_tour_flights.describe())\n",
    "\n",
    "# Example: Plotting total delay distribution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_non_tour_flights['total_delay_time'], bins=50, kde=True)\n",
    "plt.title('Distribution of Total Delay Time')\n",
    "plt.xlabel('Total Delay Time (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0976e-5633-4193-a3d9-e5f7fc431fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to get only the rows where the flight was a tour\n",
    "tour_flights = combined_data_cleaned[combined_data_cleaned['eras_tour'] == 1]\n",
    "\n",
    "# Determine the last tour date\n",
    "last_tour_date = tour_flights['date'].max()\n",
    "\n",
    "# Check the last tour date\n",
    "print(\"Last Tour Date:\", last_tour_date)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Define the start date and calculate one month after the last tour date\n",
    "start_date = pd.Timestamp('2023-05-01')\n",
    "one_month_after_last_tour = last_tour_date + pd.DateOffset(months=1)\n",
    "\n",
    "print(\"Start Date:\", start_date)\n",
    "print(\"One Month After Last Tour Date:\", one_month_after_last_tour)\n",
    "\n",
    "#Filter the DataFrame for the specified date range\n",
    "filtered_data = combined_data_cleaned[(combined_data_cleaned['date'] >= start_date) & \n",
    "                                       (combined_data_cleaned['date'] <= one_month_after_last_tour)]\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(\"Filtered Data Shape:\", filtered_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200485ca-8c59-4313-99ae-b2bec8774e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "filtered_data.to_csv('filtered_eras_dates_combined.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac8f2a-f4c1-450d-ab66-87bfde3653a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'filtered_eras_dates_combined.csv'\n",
    "filtered_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and the shape of the DataFrame\n",
    "print(filtered_data.head())\n",
    "print(\"Shape of the loaded DataFrame:\", filtered_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f61b2f-04cb-461c-97b0-660f0f60c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0af8c9-8d91-4f70-81e3-3cc5714759a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'date' column is already in datetime format\n",
    "if filtered_data['date'].dtype == 'object':\n",
    "    # Convert 'date' to datetime format\n",
    "    filtered_data['date'] = pd.to_datetime(filtered_data['date'])\n",
    "\n",
    "# Step 1: Identify the tour dates\n",
    "tour_dates = filtered_data[filtered_data['eras_tour'] == 1]['date'].unique()\n",
    "\n",
    "# Convert to a pandas datetime object (if not already)\n",
    "tour_dates = pd.to_datetime(tour_dates)\n",
    "\n",
    "# Print the extracted tour dates\n",
    "print(\"Tour Dates:\", tour_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142123e0-14e1-4aa1-be62-ec106dbfc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold the date ranges\n",
    "date_ranges = []\n",
    "\n",
    "for tour_date in tour_dates:\n",
    "    # Create a date range for ±3 days around each tour date\n",
    "    start_date = tour_date - pd.DateOffset(days=3)\n",
    "    end_date = tour_date + pd.DateOffset(days=3)\n",
    "    date_ranges.append((start_date, end_date))\n",
    "\n",
    "# Print the date ranges\n",
    "for start, end in date_ranges:\n",
    "    print(f\"Start: {start.date()}, End: {end.date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ebd27-d52c-491f-9073-b2493634bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3: Filter the data for these date ranges\n",
    "filtered_tour_delay_data = filtered_data[filtered_data['date'].isin(pd.date_range(start=min(start for start, end in date_ranges), end=max(end for start, end in date_ranges)))]\n",
    "\n",
    "# Step 4: Calculate delay metrics for this new filtered data\n",
    "tour_delay_metrics = {\n",
    "    'Average Delay': filtered_tour_delay_data['adjusted_elapsed_time'].mean(),\n",
    "    'Median Delay': filtered_tour_delay_data['adjusted_elapsed_time'].median(),\n",
    "    'Total Delay': filtered_tour_delay_data['adjusted_elapsed_time'].sum()\n",
    "}\n",
    "\n",
    "# Display the delay metrics\n",
    "print(\"Delay Metrics for Tour Dates ±3 Days:\")\n",
    "print(tour_delay_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c29f00-46ec-4756-b5c8-65d3b4cc3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'eras_tour' is a column in your DataFrame that contains tour dates\n",
    "last_tour_date = combined_data_cleaned['eras_tour'].max()\n",
    "\n",
    "# Check if last_tour_date is valid\n",
    "print(\"Last Tour Date:\", last_tour_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c30254-6960-42fc-beda-23e13e685753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last tour date from the 'date' column where 'eras_tour' is 1\n",
    "last_tour_date = combined_data_cleaned[combined_data_cleaned['eras_tour'] == 1]['date'].max()\n",
    "\n",
    "# Check if last_tour_date is valid\n",
    "print(\"Last Tour Date:\", last_tour_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a08d8-6606-4ae9-8a5d-dc12ad8daa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few entries in the eras_tour column\n",
    "print(combined_data_cleaned['eras_tour'].head())\n",
    "\n",
    "# Check the data type of the column\n",
    "print(\"Data type of 'eras_tour':\", combined_data_cleaned['eras_tour'].dtype)\n",
    "\n",
    "# Print unique values to see what they look like\n",
    "print(\"Unique values in 'eras_tour':\", combined_data_cleaned['eras_tour'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfbb05-5fcb-41e7-b7c8-aa8ab90b4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define tour and non-tour flights in the filtered data\n",
    "tour_flights_filtered = filtered_data[filtered_data['eras_tour'] == 1]\n",
    "non_tour_flights_filtered = filtered_data[filtered_data['eras_tour'] == 0]\n",
    "\n",
    "# Step 2: Calculate delay metrics\n",
    "delay_metrics_filtered = {\n",
    "    'Tour Dates': {\n",
    "        'Average Delay': tour_flights_filtered['adjusted_elapsed_time'].mean(),\n",
    "        'Median Delay': tour_flights_filtered['adjusted_elapsed_time'].median(),\n",
    "        'Total Delay': tour_flights_filtered['adjusted_elapsed_time'].sum()\n",
    "    },\n",
    "    'Non-Tour Dates': {\n",
    "        'Average Delay': non_tour_flights_filtered['adjusted_elapsed_time'].mean(),\n",
    "        'Median Delay': non_tour_flights_filtered['adjusted_elapsed_time'].median(),\n",
    "        'Total Delay': non_tour_flights_filtered['adjusted_elapsed_time'].sum()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the delay metrics\n",
    "delay_df_filtered = pd.DataFrame(delay_metrics_filtered).T\n",
    "print(delay_df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73227a-09c0-4940-ab02-c3c71493b0ec",
   "metadata": {},
   "source": [
    "Observations:\n",
    "Average Delay: Non-tour dates have a slightly higher average delay compared to tour dates.\n",
    "Total Delay: The total delay for non-tour dates is significantly higher, which may be due to the larger volume of flights on those dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbdea2-68bb-4bc5-bc62-145aec1145f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization of Average Delay\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=delay_df_filtered.index, y='Average Delay', data=delay_df_filtered, palette='pastel')\n",
    "plt.title('Average Delay on Tour vs Non-Tour Dates (Filtered Data)')\n",
    "plt.ylabel('Average Delay (in minutes)')\n",
    "plt.xlabel('Date Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00cc0c-776c-4445-bfa9-0c328adc7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_non_tour_flights.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad75fc-473d-446a-8da5-55c027efa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis:\n",
    "# Calculate the full correlation matrix\n",
    "correlation_matrix = filtered_non_tour_flights.corr()\n",
    "\n",
    "# Get the absolute values of the correlation matrix\n",
    "abs_corr_matrix = correlation_matrix.abs()\n",
    "\n",
    "# Sum the absolute correlations for each feature (except self-correlation)\n",
    "top_features = abs_corr_matrix.sum().nlargest(16).index  # Include one more to exclude self-correlation\n",
    "\n",
    "# Create a new correlation matrix with the top features\n",
    "top_corr_matrix = correlation_matrix.loc[top_features, top_features]\n",
    "\n",
    "# Plot the heatmap for the top correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(top_corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Top 15 Most Correlated Features Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450d6cc-63d6-43a8-af61-39667967496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the Correlation Matrix\n",
    "correlation_matrix = filtered_non_tour_flights.corr()\n",
    "\n",
    "# Step 2: Get the upper triangle of the correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Step 3: Stack the upper triangle matrix and sort by absolute correlation\n",
    "top_correlations = upper_triangle.stack().abs().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Step 4: Display the top correlations\n",
    "print(\"Top 10 Correlations:\")\n",
    "print(top_correlations)\n",
    "\n",
    "# Optional: If you want to visualize these top correlations\n",
    "top_correlations_df = top_correlations.reset_index()\n",
    "top_correlations_df.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_correlations_df, x='Correlation', y='Feature 1', hue='Feature 2', dodge=False)\n",
    "plt.title('Top 10 Correlations')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.legend(title='Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6674f23-73d6-4ab2-973b-b50f3c069af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify the tour dates\n",
    "tour_dates = filtered_data[filtered_data['eras_tour'] == 1]['date'].unique()\n",
    "\n",
    "# Step 2: Create a list to hold the date ranges\n",
    "date_ranges = []\n",
    "for tour_date in tour_dates:\n",
    "    # Create a date range for ±3 days around each tour date\n",
    "    start_date = tour_date - pd.DateOffset(days=3)\n",
    "    end_date = tour_date + pd.DateOffset(days=3)\n",
    "    date_ranges.append((start_date, end_date))\n",
    "\n",
    "# Step 3: Filter the data for these date ranges\n",
    "filtered_tour_delay_data = filtered_data[filtered_data['date'].isin(pd.date_range(start=min(start for start, end in date_ranges), end=max(end for start, end in date_ranges)))]\n",
    "\n",
    "# Step 4: Calculate delay metrics for this new filtered data\n",
    "tour_delay_metrics = {\n",
    "    'Average Delay': filtered_tour_delay_data['adjusted_elapsed_time'].mean(),\n",
    "    'Median Delay': filtered_tour_delay_data['adjusted_elapsed_time'].median(),\n",
    "    'Total Delay': filtered_tour_delay_data['adjusted_elapsed_time'].sum()\n",
    "}\n",
    "\n",
    "# Display the delay metrics\n",
    "print(\"Delay Metrics for Tour Dates ±3 Days:\")\n",
    "print(tour_delay_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ecccb-2742-4b00-9dad-c22b49d6bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_data = {\n",
    "    'Period': ['Tour Dates ±3 Days'],\n",
    "    'Average Delay': [tour_delay_metrics['Average Delay']],\n",
    "}\n",
    "\n",
    "visualization_df = pd.DataFrame(visualization_data)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='Period', y='Average Delay', data=visualization_df, palette='pastel')\n",
    "plt.title('Average Delay Around Tour Dates ±3 Days')\n",
    "plt.ylabel('Average Delay (in minutes)')\n",
    "plt.xlabel('Period')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccebd31-ead8-4a23-b40f-e5658bb96e31",
   "metadata": {},
   "source": [
    "Delay Metrics:\n",
    "Average Delay: ~130.27 minutes\n",
    "Median Delay: 117.0 minutes\n",
    "Total Delay: ~11,231,910 minutes\n",
    "Observations:\n",
    "The average delay for this range is similar to the average delay for non-tour dates calculated earlier, indicating that delays around the tour dates are substantial.\n",
    "The total delay is quite significant, reflecting a large number of flights impacted during this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ad0f8-46a6-4b27-9378-0b7b04c2d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Create a new column to indicate if a flight is within the ±3 days of tour dates\n",
    "filtered_data['is_tour_window'] = filtered_data['date'].apply(\n",
    "    lambda x: any(start <= x <= end for start, end in date_ranges)\n",
    ")\n",
    "\n",
    "# Step 2: Convert boolean to int (1 for True, 0 for False)\n",
    "filtered_data['is_tour_window'] = filtered_data['is_tour_window'].astype(int)\n",
    "\n",
    "# Step 3: Calculate correlation coefficients\n",
    "# Calculate the Pearson correlation coefficient\n",
    "pearson_corr, pearson_p_value = pearsonr(filtered_data['is_tour_window'], filtered_data['adjusted_elapsed_time'])\n",
    "\n",
    "# Calculate the Spearman correlation coefficient\n",
    "spearman_corr, spearman_p_value = spearmanr(filtered_data['is_tour_window'], filtered_data['adjusted_elapsed_time'])\n",
    "\n",
    "# Step 4: Display the results\n",
    "print(f\"Pearson Correlation: {pearson_corr}, p-value: {pearson_p_value}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr}, p-value: {spearman_p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96218a68-412c-4303-a86f-f46eed5a13d3",
   "metadata": {},
   "source": [
    "Pearson Correlation: 0.0036, p-value: 0.236\n",
    "Spearman Correlation: 0.0021, p-value: 0.481"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd20e5-f113-430d-859a-9f5e98522619",
   "metadata": {},
   "source": [
    "Both correlation coefficients are very close to zero, indicating a negligible relationship between being within the ±3-day window of tour dates and the adjusted elapsed time for delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b30b09-971d-4149-af60-be63081bfbd6",
   "metadata": {},
   "source": [
    "The p-values are above the typical significance threshold of 0.05, suggesting that the correlations observed are not statistically significant. In other words, we do not have enough evidence to conclude that there's a meaningful correlation between tour dates and delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c8310-bf80-4a2e-a09e-a585faf4137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Analysis:\n",
    "\n",
    "# Assuming filtered_data is the DataFrame where you've already defined your tour dates\n",
    "# Create a new column to indicate if a flight is within the ±3 days of tour dates\n",
    "filtered_data.loc[:, 'is_tour_window'] = filtered_data['date'].apply(\n",
    "    lambda x: any(start <= x <= end for start, end in date_ranges)\n",
    ")\n",
    "\n",
    "# Convert boolean to int (1 for True, 0 for False)\n",
    "filtered_data.loc[:, 'is_tour_window'] = filtered_data['is_tour_window'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f154a-89a7-488f-aab0-b063dd54e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78602d-9742-42a5-9977-cde59c13f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_non_tour_flights is already defined\n",
    "min_date = filtered_data['date'].min()\n",
    "max_date = filtered_data['date'].max()\n",
    "\n",
    "print(\"Minimum Date:\", min_date)\n",
    "print(\"Maximum Date:\", max_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2dfa39-9e1c-42aa-a679-35153250f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'is_tour_window' is already created in filtered_data\n",
    "# Prepare the data for regression analysis\n",
    "X = filtered_data[['is_tour_window', 'weather_delay', 'op_unique_carrier']]  # Include independent variables\n",
    "\n",
    "# Perform one-hot encoding for 'op_unique_carrier'\n",
    "X = pd.get_dummies(X, columns=['op_unique_carrier'], drop_first=True)\n",
    "\n",
    "y = filtered_data['adjusted_elapsed_time']  # Ensure this is present\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a527593-bf58-4c81-907b-6f7ac8c3863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)\n",
    "print(y.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e455629-0471-40fe-81c1-69f63cfbed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e39a8-3964-4387-a1f4-51e79ea25ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.to_numeric(filtered_data['adjusted_elapsed_time'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7d2b6-1528-48dc-93a0-b1e6b569dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661f21b-e5e5-43f5-8cf2-568e1818107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([X, y], axis=1).dropna()\n",
    "X = combined_data.drop(columns=[y.name])\n",
    "y = combined_data[y.name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a548e55-dc19-489c-a68f-65ba10d2c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the data for regression analysis\n",
    "X = filtered_data[['is_tour_window', 'weather_delay', 'op_unique_carrier']]  # Include independent variables\n",
    "\n",
    "# Perform one-hot encoding for 'op_unique_carrier'\n",
    "X = pd.get_dummies(X, columns=['op_unique_carrier'], drop_first=True)\n",
    "\n",
    "# Ensure all variables are in the correct format\n",
    "X['weather_delay'] = pd.to_numeric(X['weather_delay'], errors='coerce')\n",
    "y = pd.to_numeric(filtered_data['adjusted_elapsed_time'], errors='coerce')\n",
    "\n",
    "# Check for NaN values in y\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Drop NaN values\n",
    "combined_data = pd.concat([X, y], axis=1).dropna()\n",
    "X = combined_data.drop(columns=[y.name])\n",
    "y = combined_data[y.name]\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcade4-94e1-4a78-8e60-35e03f143bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)\n",
    "print(y.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec2f2d-c784-478a-8bc2-9678882e4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e18ff4-bdea-4648-ba86-4d79d6f17da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7ac2a-7288-4935-9667-65ce7ce94f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the data for regression analysis\n",
    "X = filtered_data[['is_tour_window', 'weather_delay', 'op_unique_carrier']]  # Include independent variables\n",
    "\n",
    "# Perform one-hot encoding for 'op_unique_carrier'\n",
    "X = pd.get_dummies(X, columns=['op_unique_carrier'], drop_first=True)\n",
    "\n",
    "# Check for NaN values\n",
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Drop or fill NaN values\n",
    "X = X.dropna()  # Alternatively, you can use X = X.fillna(0)\n",
    "\n",
    "# Align y with the cleaned X\n",
    "y = pd.to_numeric(filtered_data['adjusted_elapsed_time'], errors='coerce')\n",
    "y = y[X.index]  # Ensure y corresponds to the rows kept in X\n",
    "\n",
    "# Check for NaN values again\n",
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Convert to integers after handling NaNs\n",
    "X = X.astype(int)\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22783e25-ad2b-4583-87cb-6d21ce6f440b",
   "metadata": {},
   "source": [
    "Model Summary:\n",
    "R-squared: 0.114\n",
    "This means that approximately 11.4% of the variance in adjusted_elapsed_time can be explained by the model. This is relatively low, indicating that other factors not included in the model may influence the delays.\n",
    "F-statistic: 130.6 (p-value: 0.00)\n",
    "The overall model is statistically significant, indicating that at least one of the predictors is significantly related to the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564f7d6-2da4-4f60-9370-2efa51b22240",
   "metadata": {},
   "source": [
    "Coefficients Interpretation:\n",
    "is_tour_window: Coefficient = 2.7664 (p-value = 0.011)\n",
    "\n",
    "The positive coefficient suggests that being within the tour window is associated with an increase in adjusted elapsed time by about 2.77 minutes on average, which is statistically significant.\n",
    "weather_delay: Coefficient = -0.0907 (p-value = 0.022)\n",
    "\n",
    "This negative coefficient implies that for each additional minute of weather delay, the adjusted elapsed time decreases by about 0.09 minutes. This might seem counterintuitive but could indicate how weather-related delays are factored in or reported.\n",
    "Airline Coefficients: Various airlines show different effects on the adjusted elapsed time:\n",
    "\n",
    "For example, op_unique_carrier_aa has a coefficient of 41.44, meaning flights from American Airlines tend to have longer adjusted elapsed times compared to the baseline carrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569430b-77ce-49bb-a02d-dcea760e42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3dec9-0a08-4b61-82f2-dccd7b087fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Group delays by airline carrier\n",
    "grouped_delays = [group['adjusted_elapsed_time'].values for name, group in combined_data_cleaned.groupby('op_unique_carrier')]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "anova_result = f_oneway(*grouped_delays)\n",
    "\n",
    "# Print the results\n",
    "print(f\"ANOVA F-statistic: {anova_result.statistic}, p-value: {anova_result.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b03b75-c746-4547-bbc4-cdbd3f122f1a",
   "metadata": {},
   "source": [
    "F-statistic: A large F-statistic indicates that the variability explained by your model is significantly greater than the variability unexplained (the residual variance). This suggests that the model fits the data well.\n",
    "p-value: A p-value of 0.0 (essentially) means that the null hypothesis (which states that all coefficients are equal to zero) can be rejected. This suggests that at least one of your predictors is significantly related to the delays.\n",
    "Review Coefficients: Look at the individual coefficients from your regression output to see which predictors are significant and how they influence the dependent variable.\n",
    "Model Improvement: Consider adding or transforming additional predictors based on your analysis of significance and model fit.\n",
    "Residual Analysis: Continue examining residuals for any patterns to ensure model assumptions are met.\n",
    "Further Testing: Test for interaction effects or more complex models if there are theoretical reasons to believe that relationships may not be linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef975799-1c9a-4737-a9d5-2352870563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Clean the data\n",
    "filtered_data = filtered_data.dropna()  # or use fillna() as needed\n",
    "\n",
    "# Prepare the data\n",
    "X = filtered_data[['is_tour_window', 'weather_delay', 'distance']]  # Add more predictors if necessary\n",
    "X = pd.get_dummies(X, drop_first=True)  # One-hot encoding for categorical variables\n",
    "y = filtered_data['adjusted_elapsed_time']  # Dependent variable\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the regression summary\n",
    "print(model.summary())\n",
    "\n",
    "# Conduct ANOVA\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "anova_results = anova_lm(model)\n",
    "print(anova_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be250b-f585-41cc-b5f8-516802ef3f49",
   "metadata": {},
   "source": [
    "R-squared and Adjusted R-squared:\n",
    "\n",
    "R-squared: 0.864 suggests that approximately 86.4% of the variability in the adjusted_elapsed_time can be explained by the model. This is a strong indicator of a good fit.\n",
    "Adjusted R-squared: 0.863 adjusts for the number of predictors in the model, indicating that adding or removing predictors doesn't lead to overfitting.\n",
    "\n",
    "Coefficients:\n",
    "\n",
    "Intercept (const): 48.0038 suggests the baseline time when all predictors are zero.\n",
    "is_tour_window: The coefficient is 1.2541, but with a p-value of 0.282, it's not statistically significant at the 0.05 level. This means being in a tour window does not have a statistically significant effect on the adjusted elapsed time in this model.\n",
    "weather_delay: The coefficient is -0.0513, also not statistically significant (p = 0.377), indicating that weather delays do not have a meaningful impact on elapsed time in this context.\n",
    "distance: The coefficient of 0.1135 is highly significant (p < 0.001), meaning that for every unit increase in distance, the adjusted elapsed time increases significantly. This aligns with intuitive expectations that longer flights take more time.\n",
    "\n",
    "Statistical Significance:\n",
    "\n",
    "Only distance shows strong significance, while the other predictors do not.\n",
    "Model Fit and Assumptions:\n",
    "\n",
    "The F-statistic is very high (2659) with a p-value of 0.00, indicating that at least one predictor is significantly related to the response variable.\n",
    "Normality of Residuals: The Omnibus test, Jarque-Bera test, and the skewness indicate possible deviations from normality. This could suggest a need to inspect residuals further for patterns or outliers.\n",
    "Durbin-Watson statistic: Close to 2 indicates little autocorrelation in residuals, which is a good sign.\n",
    "\n",
    "Multicollinearity:\n",
    "\n",
    "The warning about a large condition number (2.70e+03) suggests potential multicollinearity among your predictors. Check the Variance Inflation Factor (VIF) for your predictors to assess multicollinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082f928-95e4-4e1a-b91f-5a43758ad75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a2e5b-5c53-4d62-a3cc-49fff57fb159",
   "metadata": {},
   "source": [
    "Since all the VIF values forpredictors (excluding the constant) are around 1, this suggests that there are no issues with multicollinearity among them. I can confidently interpret the coefficients without concern for inflated standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9500e74-31bc-4bed-9eff-83fcf9770488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# residual plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Residual plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=model.fittedvalues, y=residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e4d97-fbdd-46b1-8464-b4db4051e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Q-Q plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sm.qqplot(residuals, line='s', ax=plt.gca())\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27ff22-17fc-494a-b56b-819394d451c0",
   "metadata": {},
   "source": [
    "# Q-Q Plot: Check how closely the points follow the diagonal line. Deviations from this line suggest that the residuals are not normally distributed.\n",
    "\n",
    "Next Steps non-normality, consider transformations of the dependent variable or adding interaction terms or polynomial features.\n",
    "You can also apply statistical tests for normality (like the Shapiro-Wilk test) or homoscedasticity (like Breusch-Pagan test) for a more formal assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e0639-1bd4-463d-9c80-7063e870c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformation: Useful for positively skewed data.\n",
    "import numpy as np\n",
    "\n",
    "# Apply log transformation to the dependent variable\n",
    "combined_data_cleaned['log_adjusted_elapsed_time'] = np.log(combined_data_cleaned['adjusted_elapsed_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09689817-aeba-4884-b328-e1d8f90e72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square root transformation\n",
    "combined_data_cleaned['sqrt_adjusted_elapsed_time'] = np.sqrt(combined_data_cleaned['adjusted_elapsed_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb0e4b-5074-4649-b4c5-9ee6fea01f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox transformation\n",
    "from scipy import stats\n",
    "\n",
    "combined_data_cleaned['boxcox_adjusted_elapsed_time'], _ = stats.boxcox(combined_data_cleaned['adjusted_elapsed_time'] + 1)  # Add 1 to avoid zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8e06d-1ba9-4d59-8582-843945e42cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the data for regression analysis\n",
    "X = filtered_data[['is_tour_window', 'weather_delay', 'op_unique_carrier']]  # Include independent variables\n",
    "\n",
    "# Perform one-hot encoding for 'op_unique_carrier'\n",
    "X = pd.get_dummies(X, columns=['op_unique_carrier'], drop_first=True)\n",
    "\n",
    "# Check for NaN values\n",
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Drop or fill NaN values\n",
    "X = X.dropna()  # Alternatively, you can use X = X.fillna(0)\n",
    "\n",
    "# Align y with the cleaned X\n",
    "y = pd.to_numeric(filtered_data['adjusted_elapsed_time'], errors='coerce')\n",
    "y = y[X.index]  # Ensure y corresponds to the rows kept in X\n",
    "\n",
    "# Check for NaN values again\n",
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Convert to integers after handling NaNs\n",
    "X = X.astype(int)\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cb209-808c-4491-ba2f-3b7fece0433c",
   "metadata": {},
   "source": [
    "R-squared Value: The R-squared value has increased to around 0.171, which indicates that the model explains about 17.1% of the variance in the adjusted elapsed time. While this is an improvement, it still suggests that there might be other important predictors not included in the model. Adj. R-squared: 0.161\n",
    "This value adjusts for the number of predictors in the model, providing a more accurate measure when comparing models with different numbers of predictors.\n",
    "\n",
    "F-statistic: 17.12\n",
    "\n",
    "The associated p-value (1.74e-41) indicates that the model as a whole is statistically significant, meaning at least one of the predictors is significantly related to the response variable.\n",
    "\n",
    "\n",
    "Significant Predictors: Look closely at the coefficients and their p-values. For instance, the variables op_unique_carrier_b6, op_unique_carrier_f9, and others show significant coefficients (p < 0.05). This suggests they contribute meaningfully to the model.  \n",
    "\n",
    "Coefficients:\n",
    "\n",
    "The const (intercept) is approximately 102.48, meaning that when all predictors are zero, the estimated adjusted elapsed time is about 102.48 minutes.\n",
    "The coefficients for the independent variables tell you how much the adjusted elapsed time is expected to increase (or decrease) with a one-unit increase in each predictor, holding all other predictors constant.\n",
    "Key Predictors:\n",
    "\n",
    "op_unique_carrier_aa: 22.46 (p < 0.001)\n",
    "op_unique_carrier_b6: 57.98 (p < 0.001)\n",
    "op_unique_carrier_nk: 59.46 (p < 0.001)\n",
    "weather_delay: 0.0029 (p = 0.984)\n",
    "The weather delay variable is not statistically significant, indicating that it does not contribute to explaining variations in adjusted elapsed time.\n",
    "Airline Effects:\n",
    "\n",
    "Several airlines (like B6, NK, and UA) show significant positive coefficients, suggesting that flights operated by these airlines tend to have longer delays compared to the reference category (which was dropped during one-hot encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c13d63-d018-40b3-9805-ff166f377415",
   "metadata": {},
   "source": [
    " Check for Multicollinearity\n",
    "Although your VIF results from previous models showed no severe multicollinearity, keep an eye on the correlation between the one-hot encoded variables. High correlation could still affect the model.\n",
    "Continue analyzing the residuals to check for homoscedasticity and normality. You previously mentioned using Q-Q plots; you can also employ residual plots to visualize the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe7141-004a-41b5-ac5b-6caf98400a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(model.fittedvalues, model.resid)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Fitted')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sm.qqplot(model.resid, line='s', ax=plt.gca())\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c28d14-0cc5-4bc4-bf9a-385bacbbf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2837f73-5297-47e5-b63d-118a2d96a51d",
   "metadata": {},
   "source": [
    "High VIF:\n",
    "The constant term has a high VIF (18.10), which is expected, as it's a result of being included in the model.\n",
    "The variable op_unique_carrier_aa shows a VIF of 3.11, which is still within acceptable limits but worth monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949c8bd-0d7e-4d93-9792-01330707f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation\n",
    "combined_data_cleaned['log_adjusted_elapsed_time'] = np.log(combined_data_cleaned['adjusted_elapsed_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b712d9-a643-41d0-a9cd-b26e46b31d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Apply Box-Cox transformation (ensure there are no zero or negative values)\n",
    "combined_data_cleaned['boxcox_adjusted_elapsed_time'], _ = stats.boxcox(combined_data_cleaned['adjusted_elapsed_time'] + 1)  # Adding 1 to avoid zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496c6ea-e414-48ce-b92b-15f307dbb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a52d7-e48b-4abd-90ed-75ee58cba364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for regression analysis\n",
    "y = combined_data_cleaned['log_adjusted_elapsed_time']\n",
    "X = filtered_[['is_tour_window', 'weather_delay', 'op_unique_carrier']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7010c34-178b-4d20-90d5-a527d55e45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encoding for 'op_unique_carrier'\n",
    "X = pd.get_dummies(X, columns=['op_unique_carrier'], drop_first=True)\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "X = X.dropna()\n",
    "y = y[X.index]  # Align y with X\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede6cf6-dc28-484b-8cfb-5340f1363ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available columns first\n",
    "print(combined_data_cleaned.columns)\n",
    "\n",
    "# Prepare the data for regression analysis without 'is_tour_window'\n",
    "y = combined_data_cleaned['log_adjusted_elapsed_time']\n",
    "X = combined_data_cleaned[['weather_delay', 'op_unique_carrier']]  # Remove 'is_tour_window' if it doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebfa441-68a7-4863-9afd-c6d78b474118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create is_tour_window if needed\n",
    "combined_data_cleaned['is_tour_window'] = combined_data_cleaned['is_weekend']  # Example condition\n",
    "\n",
    "# Prepare the data for regression analysis\n",
    "y = combined_data_cleaned['log_adjusted_elapsed_time']\n",
    "X = combined_data_cleaned[['is_tour_window', 'weather_delay', 'op_unique_carrier']]\n",
    "\n",
    "# Perform one-hot encoding for 'op_unique_carrier'\n",
    "X = pd.get_dummies(X, columns=['op_unique_carrier'], drop_first=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Drop rows with NaNs\n",
    "X = X.dropna()\n",
    "y = y[X.index]  # Align y with the cleaned X\n",
    "\n",
    "# Fit the model\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)  # Add a constant for intercept\n",
    "model = sm.OLS(y, X).fit()  # Fit the model\n",
    "print(model.summary())  # Print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298d72e-810e-4b9d-9853-25966d31cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)\n",
    "print(y.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679cd2f-7422-4b59-a61d-b4707990dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3e601-1d72-465c-a5da-da9c10aa6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b39dee-91b4-4bae-b911-8e55dcb7a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd6daa-7708-47a2-881b-f2b0548bac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the model (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7da286-aea9-4d36-8b1f-3f734b216a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Prepare the data for VIF calculation\n",
    "X_vif = combined_data_cleaned[['is_tour_window', 'weather_delay'] + \n",
    "                                [col for col in combined_data_cleaned.columns if col.startswith('op_unique_carrier_')]]\n",
    "\n",
    "# Add a constant for the intercept\n",
    "X_vif = sm.add_constant(X_vif)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X_vif.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7adc71-63de-4829-9544-2a85eff2f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the predictors\n",
    "missing_values = X_vif.isnull().sum()\n",
    "print(\"Missing values in each feature:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5993026-ec74-43b0-bd93-6c7981431843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of weather_delay values\n",
    "print(combined_data_cleaned['weather_delay'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b57f66-a40a-48cc-a981-ff68fcdf0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_weather_delay = X_vif['weather_delay'].median()\n",
    "X_vif['weather_delay'].fillna(median_weather_delay, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e11bdf-d05e-4e70-8f26-a6c68826a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values again\n",
    "print(X_vif.isnull().sum())\n",
    "\n",
    "# If all missing values are addressed, add a constant for the intercept\n",
    "import statsmodels.api as sm\n",
    "X_vif = sm.add_constant(X_vif)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X_vif.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c931a-e9bc-4f78-9ad9-bae0f050c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f989b9-ee12-4a26-b974-c37341d86563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the median for numerical columns\n",
    "X_train.fillna(X_train.median(), inplace=True)\n",
    "y_train.fillna(y_train.median(), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b621bc5-c688-4f3c-a57d-9c5b58b153c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())\n",
    "print(y_train.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501c39f-f05c-474d-9255-98c1269ce7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant for intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the model\n",
    "baseline_model = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(baseline_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174391e-b554-4efd-b091-b3d4b790b5d8",
   "metadata": {},
   "source": [
    "R-squared: 0.000, which means the model explains none of the variance in the dependent variable. This suggests that the predictors used (weather delay and is_tour_window) do not effectively capture the variation in the response variable.\n",
    "F-statistic: 783.5 with a p-value of 0.00 indicates that at least one predictor is statistically significant in explaining the variance, but it doesn't tell us about the overall fit of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cdce72-7d43-4470-8ee7-0fa59842b5df",
   "metadata": {},
   "source": [
    "\n",
    "The regression results you provided indicate a few key points about your model:\n",
    "\n",
    "Model Summary\n",
    "Dependent Variable: log_adjusted_elapsed_time\n",
    "R-squared: 0.000, which means the model explains none of the variance in the dependent variable. This suggests that the predictors used (weather delay and is_tour_window) do not effectively capture the variation in the response variable.\n",
    "F-statistic: 783.5 with a p-value of 0.00 indicates that at least one predictor is statistically significant in explaining the variance, but it doesn't tell us about the overall fit of the model.\n",
    "Coefficients\n",
    "Intercept (const): 4.7850, which is the expected log-adjusted elapsed time when all predictors are zero.\n",
    "weather_delay: Coefficient is 0.0002, meaning that for every unit increase in weather delay, the log-adjusted elapsed time increases by 0.0002 units. It is statistically significant (p-value = 0.000).\n",
    "is_tour_window: Coefficient is 0.0141, suggesting that being in a tour window is associated with an increase in log-adjusted elapsed time by 0.0141 units. This is also statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9988fe-f01a-4529-a706-f499a9a1ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `combined_data_cleaned` is your DataFrame\n",
    "combined_data_cleaned = pd.get_dummies(combined_data_cleaned, columns=['op_unique_carrier', 'day_of_week'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d8805-3f79-46f0-b1ca-b3128847b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_data_cleaned['log_adjusted_elapsed_time']\n",
    "\n",
    "X = combined_data_cleaned[['is_tour_window', 'weather_delay', 'distance'] + \n",
    "                           [col for col in combined_data_cleaned.columns if col.startswith('op_unique_carrier_')] + \n",
    "                           [col for col in combined_data_cleaned.columns if col.startswith('day_of_week_')]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281ebd8-8354-4a6f-8446-0b8e8c564b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add constant for intercept\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y_train, X_train_sm).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae0f80-dbf3-4671-b4ea-4327b3d37ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
