{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95234c0-029c-4f0a-bf70-88bd88e57e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e5310-d1fd-497b-bc5f-93c7c4db1003",
   "metadata": {},
   "source": [
    "Combine all files in weather_data folder into one .csv file titled combined_weather.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a39f60-1a6e-4cc0-983e-fb8b1618ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the folder containing the CSV files and the output file path\n",
    "input_folder = r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\weather_data'\n",
    "output_file = r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\combined_weather.csv'\n",
    "\n",
    "\n",
    "# List to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        # Add an ID column with the filename as the ID\n",
    "        df['ID'] = file_name\n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153916b1-8783-4b03-8150-917ccf4b112b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m combined_weather_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhopeh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata_science_bootcamp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mflight_times_capstone\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcombined_weather.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m weather_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(combined_weather_path, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the DataFrame\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(weather_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "# Define the path to the combined CSV file\n",
    "combined_weather_path = r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\combined_weather.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "weather_df = pd.read_csv(combined_weather_path, low_memory=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2fdce-db2a-45e3-9832-90fa16c4168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names\n",
    "print(weather_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1ac9d-2a2c-4f00-af33-026013745956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the DataFrame\n",
    "print(weather_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24de5d-8e2c-46d0-83d3-3c9ed74113ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce memory usage\n",
    "\n",
    "# Convert float64 to float32\n",
    "float_cols = weather_df.select_dtypes(include=['float64']).columns\n",
    "weather_df[float_cols] = weather_df[float_cols].astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ed952-fbfe-4157-b0a8-e4fa4e247a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "\n",
    "# Find all duplicate rows where all cells are identical\n",
    "duplicate_rows = weather_df[weather_df.duplicated(keep=False)]\n",
    "\n",
    "# Display the first 10 unique duplicate rows\n",
    "unique_duplicate_rows = duplicate_rows.drop_duplicates()\n",
    "print(unique_duplicate_rows.head(10))\n",
    "\n",
    "# Count the number of unique duplicate rows\n",
    "unique_duplicate_count = unique_duplicate_rows.shape[0]\n",
    "print(f\"Total unique duplicate rows: {unique_duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03585f49-ac88-449a-901d-a335c70523ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat column titles\n",
    "\n",
    "# Convert all column names to lowercase\n",
    "weather_df.columns = weather_df.columns.str.lower()\n",
    "\n",
    "# # Rename specific columns if needed\n",
    "# weather_df.rename(columns={\n",
    "#     'fl_date': 'date', \n",
    "#     'origin_city_name': 'origin_city',\n",
    "#     'dest_city_name' : 'dest_city'\n",
    "# }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d232afe-519b-4c1f-bfd6-a2f021305faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(weather_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec651f76-ff64-41d0-bb77-feb5f7358a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see difference\n",
    "print(weather_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a508724-50d0-4c78-ba00-102cc27f234f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(weather_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424688a-ecff-40f3-989d-3e84907abeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the missing value threshold to determine which attributes to remove (20%)\n",
    "# threshold = 0.20\n",
    "\n",
    "# # Calculate the proportion of missing values\n",
    "# missing_proportion = combined_weather_df.isnull().mean()\n",
    "\n",
    "# # Calculate the proportion of zero values\n",
    "# zero_proportion = (combined_weather_df == 0).mean()\n",
    "\n",
    "# # Filter columns where missing or zero values are more than the threshold\n",
    "# high_missing_cols = missing_proportion[missing_proportion > threshold].index\n",
    "# high_zero_cols = zero_proportion[zero_proportion > threshold].index\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Columns with more than 20% missing values:\")\n",
    "# print(high_missing_cols)\n",
    "\n",
    "# print(\"\\nColumns with more than 20% zero values:\")\n",
    "# print(high_zero_cols)\n",
    "\n",
    "# # Combine both lists to see columns with either high missing or zero values\n",
    "# high_issue_cols = set(high_missing_cols).union(set(high_zero_cols))\n",
    "# print(\"\\nColumns with either high missing or zero values:\")\n",
    "# print(high_issue_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736f933-697b-4535-995d-88337f792232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of NaN values for each column\n",
    "nan_percentage = weather_df.isna().mean() * 100\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "\n",
    "# Print the results\n",
    "print(nan_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04ec86-6639-4295-9bc5-f5f8b402533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the subset of columns to KEEP\n",
    "columns_to_keep = ['station', 'id', 'name', 'latitude', 'longitude', 'elevation', 'date', 'prcp', 'snow', 'snwd', 'tmax', 'tmin', 'tobs']\n",
    "weather_df = weather_df[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a09cd-51ee-42a2-b22a-60e32daed912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop Columns that provide no useful information for analysis (e.g., unique\n",
    "# # identifiers if not needed)\n",
    "\n",
    "# columns_to_drop = [\n",
    "#     'origin_city_market_id',\n",
    "#     'dest_city_market_id',\n",
    "#     'origin',\n",
    "#     'dest',\n",
    "#     'origin_state_nm',\n",
    "#     'dest_state_nm', \n",
    "#     'origin_airport_id',\n",
    "#     'dest_airport_id'\n",
    "# ]\n",
    "\n",
    "# # Drop the identified columns\n",
    "# flight_data = flight_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# # Verify the changes\n",
    "# print(flight_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f249b-9d5d-439e-a5c0-b4be2996c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of NaN values per row\n",
    "num_nans_per_row = weather_df.isnull().sum(axis=1)\n",
    "\n",
    "# Calculate the total number of columns in the DataFrame\n",
    "total_columns = weather_df.shape[1]\n",
    "\n",
    "# Calculate the proportion of NaN values per row\n",
    "nan_proportion_per_row = num_nans_per_row / total_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62cf4b-0521-40dc-b4c4-c859772c5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold (62%)\n",
    "threshold = 0.62\n",
    "\n",
    "# Identify rows where the proportion of NaN values is more than the threshold\n",
    "rows_with_high_nan = weather_df[nan_proportion_per_row > threshold]\n",
    "\n",
    "# Save the rows with high NaN proportions to a new CSV file\n",
    "rows_with_high_nan.to_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\rows_with_high_nan.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a74b48-f3ab-48c8-a4c1-280450c0a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows with more than 62% NaN values\n",
    "print(f\"Number of rows with more than 62% NaN values: {len(rows_with_high_nan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039ca40-f3a3-4586-9da0-2ee0575cf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "rows_w_high_nan_df = pd.read_csv(r'C:\\Users\\hopeh\\Desktop\\data_science_bootcamp\\flight_times_capstone\\rows_with_high_nan.csv', low_memory=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(rows_w_high_nan_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a588cdb-a8e7-4a80-9aab-736f8e92bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names\n",
    "print(rows_w_high_nan_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b16984-1141-4323-b34d-60fa689219c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to check for NaN\n",
    "columns_to_check = ['latitude', 'longitude', 'elevation', 'prcp', 'snow', 'snwd', 'tmax', 'tmin', 'tobs']\n",
    "\n",
    "# Remove rows where all specified columns are NaN\n",
    "weather_df = weather_df.dropna(subset=columns_to_check, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c6d9-5bc8-4266-b602-dacaa19bc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'name' column into 'city' and 'state'\n",
    "weather_df[['city', 'state']] = weather_df['name'].str.split(',', n=1, expand=True)\n",
    "\n",
    "# Clean up the 'city' and 'state' columns\n",
    "weather_df.loc[:, 'city'] = weather_df['city'].str.replace(r'\\d+\\.?\\d*\\s+[NSEW]+', '', regex=True).str.strip()\n",
    "weather_df.loc[:, 'state'] = weather_df['state'].str.extract(r'([A-Z]{2})')[0]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216edbb-e991-4a3a-872d-c1b501309511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the cleaned DataFrame\n",
    "print(f'DataFrame shape: {weather_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf0e4e-6ba3-46c5-a910-72a689d20fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the name column\n",
    "weather_df.drop(columns=['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ba0ff-a0fd-4cad-a2cb-c9d77ed81267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca69fc1-f969-4e28-a0c0-f985f6c92cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = '2023-04-30'\n",
    "end_date = '2024-04-30'\n",
    "\n",
    "# Filter the DataFrame\n",
    "weather_df = weather_df[(weather_df['date'] >= start_date) & (weather_df['date'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a9571-c686-4cc7-b6ca-9874356e88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5d7c8-7fab-47d0-abfa-76f3c6436c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in the DataFrame to lowercase using apply\n",
    "weather_df = weather_df.apply(lambda col: col.str.lower() if col.dtype == \"object\" else col)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(weather_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959163e1-791c-4bcb-a77b-2c4747dbad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned DataFrame to a new CSV file\n",
    "weather_df.to_csv('cleaned_weather_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a75f39-45be-4e8e-9118-b3ee29562358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names\n",
    "print(weather_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f3697-40fd-4cca-a8b9-f22aead63a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
